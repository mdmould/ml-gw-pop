{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05629f79-710d-42f1-a2ff-ba19ae032ab7",
   "metadata": {},
   "source": [
    "If you're running in a separate notebook (e.g., Google Colab), go through and un-comment the cells below as required. Also make sure to set the runtime before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef958a-7187-4607-93a5-e025b1d5b5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're running on a shared cluster and want to limit the resources you take up:\n",
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = '1'\n",
    "os.environ[\"MKL_NUM_THREADS\"] = '1'\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = '1'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = '1'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['NPROC'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # you can change to a GPU ID not in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee84ca38-867c-4a27-b469-7bb8f11ce30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy matplotlib corner h5ify\n",
    "# !pip install wcosmo jax_tqdm equinox equinox optax flowjax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755fdc9a-fa7d-4ae3-83da-2e7f49b8c76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you're running on CPU:\n",
    "# !pip install jax numypro\n",
    "\n",
    "# # If you're running on GPU\n",
    "# !pip install -U 'jax[cuda12]'\n",
    "# !pip install 'numpyro[cuda]' -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ef9666-6632-41e5-b3ae-296df3530963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download software injections and parameter estimation from LVK O3:\n",
    "# !mkdir -p data\n",
    "# !wget https://github.com/mdmould/ml-gw-pop/raw/refs/heads/main/data/vt.h5 -P data\n",
    "# !wget https://github.com/mdmould/ml-gw-pop/raw/refs/heads/main/data/pe.h5 -P data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b7abc-c905-476e-9510-0eac7d81ad69",
   "metadata": {},
   "source": [
    "## Flow-based variational inference for gravitational-wave populations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40221c43-7a01-4d01-a75a-739acfb5966c",
   "metadata": {},
   "source": [
    "In this notebook, we'll train a normalizing flow to learn the Bayesian posterior for an astrophysical population model from gravitational-wave catalogues using variational inference. We'll focus on just binary black-hole mergers. We'll use [JAX](https://github.com/jax-ml/jax) as the main workhorse behind this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a6f54-9afa-4a25-9f2f-3918df0de1a3",
   "metadata": {},
   "source": [
    "This is based on our recent work, [\"Rapid inference and comparison of gravitational-wave population models with neural variational posteriors\" (arXiv:2504.07197)](https://arxiv.org/abs/2504.07197), for which there is also some code available in the [gwax](https://github.com/mdmould/gwax) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba92f396-e8d9-4eff-aecd-d954c2e7e9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "jax.config.update('jax_enable_x64', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8872384d-c062-4889-9bdc-30841d4dcb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for GPU devices\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5021bc7b-3955-4984-b210-ba2dfc0f26d6",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d066b-920b-4bfd-9816-c96662a2907b",
   "metadata": {},
   "source": [
    "We will perform population inference on the catalogue of black-hole mergers with false-alarm rates > 1/year from O3. Below, we load in pre-prepared parameter estimation results for those events and a set of software injections that we can use to estimate selection effects (the scripts in the `data/` folder were used to download and prepare the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb19e09b-0c0f-4755-a053-1865223e1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5ify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3add73-5f25-4560-9a9e-16166847153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "injections = h5ify.load('data/vt.h5')\n",
    "injections = {\n",
    "    k: jnp.array(injections[k], dtype = jnp.float64).squeeze()\n",
    "    for k in injections\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96273869-ae8e-4c2e-b5e4-436a2376e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "posteriors = h5ify.load('data/pe.h5')\n",
    "posteriors = {\n",
    "    k: jnp.array([posteriors[event][k] for event in sorted(posteriors)])\n",
    "    for k in posteriors[list(posteriors)[0]]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3a0074-b21d-4189-bf8a-0404ffc8dd05",
   "metadata": {},
   "source": [
    "#### Population model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082b62e-1b48-4aeb-bcc0-079d0775a122",
   "metadata": {},
   "source": [
    "First, let's define the population model that we'll use for the astrophysical distribution of sources. We'll include source-frame primary masses, binary mass ratio, dimensionless spin magnitudes, spin-orbit misalignments, and redshift.\n",
    "\n",
    "- The primary masses and mass ratios will follow my version [Power Law + Peak](https://arxiv.org/abs/1801.02699) model, which has tapering functions at low and high black-hole masses with analytical normalization.\n",
    "- Spin magnitudes will be fit with a truncated normal distribution, independent and identical between primary and secondary black holes.\n",
    "- Ditto for spin tilts.\n",
    "- We'll assume that the merger rate evolves over comoving volume and source-frame time as a [power law in redshift](https://arxiv.org/abs/1805.10270).\n",
    "\n",
    "The key thing to remember is that the likelihood function - and thus the population model - *must* be automatically differentiable. To be compatible with the framework here, it must be coded in JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f999ec94-e0d3-4441-a354-138efea1c3fc",
   "metadata": {},
   "source": [
    "We'll also use [wcosmo](https://github.com/ColmTalbot/wcosmo), which is a nice package for cosmological calculations in JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b8983-7273-4fb2-aaab-b88c83abb6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wcosmo\n",
    "wcosmo.disable_units()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753b77c2-c34d-4bae-a3e6-d035daac5c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tapering functions\n",
    "\n",
    "def cubic_filter(x):\n",
    "    return (3 - 2 * x) * x**2 * (0 <= x) * (x <= 1) + (1 < x)\n",
    "\n",
    "def highpass(x, xmin, dmin):\n",
    "    return cubic_filter((x - xmin) / dmin)\n",
    "\n",
    "def lowpass(x, xmax, dmax):\n",
    "    return highpass(x, xmax, -dmax)\n",
    "\n",
    "def bandpass(x, xmin, xmax, dmin, dmax):\n",
    "    return highpass(x, xmin, dmin) * lowpass(x, xmax, dmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5c0b94-5be2-41d9-b5fe-708c362afb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# power law functions\n",
    "\n",
    "def powerlaw(x, alpha, xmin, xmax):\n",
    "    cut = (xmin <= x) * (x <= xmax)\n",
    "    shape = x**alpha\n",
    "    norm = (xmax**(alpha + 1) - xmin**(alpha + 1)) / (alpha + 1)\n",
    "    return cut * shape / norm\n",
    "\n",
    "def powerlaw_integral(x, alpha, loc, delta):\n",
    "    a, c, d = alpha, loc, delta\n",
    "    return (\n",
    "        3 * (2 * c + (4 + a) * d)\n",
    "        * (c**2 / (1 + a) - 2 * c * x / (2 + a) + x**2 / (3 + a))\n",
    "        - 2 * (x - c)**3\n",
    "    ) * x**(1 + a) / (4 + a) / d**3\n",
    "\n",
    "def highpass_powerlaw_integral(x, alpha, xmin, xmax, dmin):\n",
    "    return (\n",
    "        (\n",
    "            - powerlaw_integral(xmin, alpha, xmin, dmin)\n",
    "            + powerlaw_integral(jnp.minimum(xmin + dmin, x), alpha, xmin, dmin)\n",
    "        ) * (xmin <= x)\n",
    "        + (\n",
    "            - (xmin + dmin)**(alpha + 1) / (alpha + 1)\n",
    "            + xmax**(alpha + 1) / (alpha + 1)\n",
    "        ) * (xmin + dmin <= x)\n",
    "    )\n",
    "\n",
    "def highpass_powerlaw(x, alpha, xmin, xmax, dmin):\n",
    "    cut = (xmin <= x) * (x <= xmax)\n",
    "    shape = x**alpha * highpass(x, xmin, dmin)\n",
    "    norm = highpass_powerlaw_integral(xmax, alpha, xmin, xmax, dmin)\n",
    "    return cut * shape / norm\n",
    "\n",
    "def bandpass_powerlaw(x, alpha, xmin, xmax, dmin, dmax):\n",
    "    cut = (xmin <= x) * (x <= xmax)\n",
    "    shape = x**alpha * bandpass(x, xmin, xmax, dmin, dmax)\n",
    "    norm = (\n",
    "        - powerlaw_integral(xmin, alpha, xmin, dmin)\n",
    "        + powerlaw_integral(xmin + dmin, alpha, xmin, dmin)\n",
    "        - (xmin + dmin)**(alpha + 1) / (alpha + 1)\n",
    "        + (xmax - dmax)**(alpha + 1) / (alpha + 1)\n",
    "        - powerlaw_integral(xmax - dmax, alpha, xmax, -dmax)\n",
    "        + powerlaw_integral(xmax, alpha, xmax, -dmax)\n",
    "    )\n",
    "    return cut * shape / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2c7762-6f2e-44e7-97cf-cb4cdb806f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian functions\n",
    "\n",
    "def truncnorm(x, mu, sigma, xmin, xmax):\n",
    "    cut = (xmin <= x) * (x <= xmax)\n",
    "    shape = jax.scipy.stats.norm.pdf(x, mu, sigma)\n",
    "    norm = (\n",
    "        - jax.scipy.stats.norm.cdf(xmin, mu, sigma)\n",
    "        + jax.scipy.stats.norm.cdf(xmax, mu, sigma)\n",
    "    )\n",
    "    return cut * shape / norm\n",
    "\n",
    "def normal_integral(x, mu, sigma, loc, delta):\n",
    "    m, s, c, d = mu, sigma, loc, delta\n",
    "    return (\n",
    "        jnp.exp(-(x - m)**2 / 2 / s ** 2) * (2 / jnp.pi)**0.5 * s * (\n",
    "            6 * c * (c + d - m - x)\n",
    "            - 3 * d * (m + x)\n",
    "            + 2 * (m**2 + 2 * s**2 + m * x + x**2)\n",
    "        )\n",
    "        - jax.lax.erf((m - x) / s / 2**0.5) * (\n",
    "            (2 * c + 3 * d - 2 * m) * (c - m)**2\n",
    "            + 3 * s**2 * (2 * c + d - 2 * m)\n",
    "        )\n",
    "    ) / 2 / d**3\n",
    "\n",
    "def bandpass_normal(x, mu, sigma, xmin, xmax, dmin, dmax):\n",
    "    cut = (xmin <= x) * (x <= xmax)\n",
    "    shape = (\n",
    "        jax.scipy.stats.norm.pdf(x, mu, sigma)\n",
    "        * bandpass(x, xmin, xmax, dmin, dmax)\n",
    "    )\n",
    "    norm = (\n",
    "        - normal_integral(xmin, mu, sigma, xmin, dmin)\n",
    "        + normal_integral(xmin + dmin, mu, sigma, xmin, dmin)\n",
    "        - jax.scipy.stats.norm.cdf(xmin + dmin, mu, sigma)\n",
    "        + jax.scipy.stats.norm.cdf(xmax - dmax, mu, sigma)\n",
    "        - normal_integral(xmax - dmax, mu, sigma, xmax, -dmax)\n",
    "        + normal_integral(xmax, mu, sigma, xmax, -dmax)\n",
    "    )\n",
    "    return cut * shape / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba8bfb2-686f-44e6-9a6d-dee731930495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary mass\n",
    "def pdf_m(m, parameters):\n",
    "    pl = bandpass_powerlaw(\n",
    "        m,\n",
    "        parameters['alpha'],\n",
    "        parameters['m_min'],\n",
    "        parameters['m_max'],\n",
    "        parameters['d_min'],\n",
    "        parameters['d_max'],\n",
    "    )\n",
    "    tn = bandpass_normal(\n",
    "        m,\n",
    "        parameters['mu_m'],\n",
    "        parameters['sigma_m'],\n",
    "        parameters['m_min'],\n",
    "        parameters['m_max'],\n",
    "        parameters['d_min'],\n",
    "        parameters['d_max'],\n",
    "    )\n",
    "    return (1 - parameters['f_m']) * pl + parameters['f_m'] * tn\n",
    "\n",
    "# mass ratio - this is a bit of a handful, but otherwise, autodiff doesn't work\n",
    "# let me know if you spot a better way to do it :')\n",
    "def pdf_q_given_m(q, m, parameters):\n",
    "    # pdf defined in terms if secondary mass, then converted to mass ratio\n",
    "    pdf = lambda q, m: highpass_powerlaw(\n",
    "        q * m, parameters['beta'], parameters['m_min'], m, parameters['d_min'],\n",
    "    ) * m\n",
    "    single = lambda q, m: jax.lax.cond(\n",
    "        parameters['m_min'] <= q * m, lambda: pdf(q, m), lambda: 0.0,\n",
    "    )\n",
    "    return jax.vmap(single)(q.ravel(), m.ravel()).reshape(q.shape)\n",
    "\n",
    "# spin magnitude\n",
    "def pdf_a(a, parameters):\n",
    "    return truncnorm(a, parameters['mu_a'], parameters['sigma_a'], 0, 1)\n",
    "\n",
    "# spin tilt\n",
    "def pdf_c(c, parameters):\n",
    "    return truncnorm(c, parameters['mu_c'], parameters['sigma_c'], -1, 1)\n",
    "\n",
    "# redshift\n",
    "def shape_z(z, parameters):\n",
    "    return (1 + z)**parameters['gamma']\n",
    "\n",
    "def pdf_z(z, parameters):\n",
    "    zmax = 2\n",
    "    fn = lambda z: (\n",
    "        shape_z(z, parameters)\n",
    "        * wcosmo.Planck15.differential_comoving_volume(z) * 4 * jnp.pi / 1e9\n",
    "    )\n",
    "    cut = (0 < z) * (z <= zmax)\n",
    "    shape = fn(z)\n",
    "    zz = jnp.linspace(0, zmax, 10_000)\n",
    "    norm = jnp.trapezoid(fn(zz), zz)\n",
    "    return cut * shape / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb1e9f-3381-4573-8fdf-9a4c37bc29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the combined probability density for the population model\n",
    "def density(data, parameters):\n",
    "    return (\n",
    "        pdf_m(data['mass_1_source'], parameters)\n",
    "        * pdf_q_given_m(data['mass_ratio'], data['mass_1_source'], parameters)\n",
    "        # * pdf_q(data['mass_ratio'], parameters)\n",
    "        * pdf_a(data['a_1'], parameters)\n",
    "        * pdf_a(data['a_2'], parameters)\n",
    "        * pdf_c(data['cos_tilt_1'], parameters)\n",
    "        * pdf_c(data['cos_tilt_2'], parameters)\n",
    "        * pdf_z(data['redshift'], parameters)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88a8379-ee9c-400e-8656-7950bd2a49be",
   "metadata": {},
   "source": [
    "Let's plot what the population models look like for some parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944182d1-2a62-4022-9900-28a453a71bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3efc544-b874-4b8e-9bbb-785be878db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict(\n",
    "    alpha = -3.5,\n",
    "    m_min = 5,\n",
    "    m_max = 80,\n",
    "    d_min = 5,\n",
    "    d_max = 10,\n",
    "    mu_m = 35,\n",
    "    sigma_m = 3,\n",
    "    f_m = 0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e926564-ca63-4c6b-8db5-51364144746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = jnp.linspace(2, 100, 1_000)\n",
    "p = pdf_m(m, parameters)\n",
    "\n",
    "plt.plot(m, p)\n",
    "plt.semilogy()\n",
    "plt.xlabel('primary mass')\n",
    "plt.ylabel('PDF')\n",
    "plt.ylim(1e-5, 1e0)\n",
    "\n",
    "print(jnp.trapezoid(p, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6682f45c-93f3-4758-9aad-ee2f391fd1e4",
   "metadata": {},
   "source": [
    "#### Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e870f9-0fcf-4c36-94f8-304978e74b93",
   "metadata": {},
   "source": [
    "Next, we'll set priors on the parameters of the population model - these are the parameters we want to measure from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3ab3e8-61f5-438a-b362-5a99de5703b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe85080-17da-4afe-8c33-a79c33437a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = dict(\n",
    "    alpha = numpyro.distributions.Uniform(-10, 10),\n",
    "    m_min = numpyro.distributions.Uniform(2, 6),\n",
    "    m_max = numpyro.distributions.Uniform(70, 100),\n",
    "    d_min = numpyro.distributions.Uniform(0, 10),\n",
    "    d_max = numpyro.distributions.Uniform(0, 10),\n",
    "    mu_m = numpyro.distributions.Uniform(20, 50),\n",
    "    sigma_m = numpyro.distributions.Uniform(1, 10),\n",
    "    f_m = numpyro.distributions.Uniform(0, 1),\n",
    "    beta = numpyro.distributions.Uniform(-10, 10),\n",
    "    mu_a = numpyro.distributions.Uniform(0, 1),\n",
    "    sigma_a = numpyro.distributions.Uniform(0.1, 1),\n",
    "    mu_c = numpyro.distributions.Uniform(-1, 1),\n",
    "    sigma_c = numpyro.distributions.Uniform(0.1, 4),\n",
    "    gamma = numpyro.distributions.Uniform(-10, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f736a16-e067-4466-8329-b5d7f6dd1d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(priors)\n",
    "dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7406fd5-831f-4b16-a3f0-d3d1475dc867",
   "metadata": {},
   "source": [
    "#### Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1651c356-e767-481e-92f5-27c5a0ce7133",
   "metadata": {},
   "source": [
    "How likely is it that our population model is responsible for the observed data?\n",
    "\n",
    "Unlike neural posterior estimation, which is a simulation-based inference method, variational inference is a likelihood-based method. This also means that it is not amortized, i.e., it is fit to one specific data set. Below we code up the gravitational-wave population likelihood; see, e.g.,\n",
    "\n",
    "- https://arxiv.org/abs/1809.02063,\n",
    "- https://arxiv.org/abs/2007.05579,\n",
    "- https://arxiv.org/abs/2410.19145.\n",
    "\n",
    "In particular, the likelihood function is approximated with several Monte Carlo integrals, which introduces additional statistical variance (https://arxiv.org/abs/1904.10879, https://arxiv.org/abs/2204.00461, https://arxiv.org/abs/2304.06138). We make sure to keep track of this variance below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787f1a08-3461-4c6e-8900-6cc07b2e89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and variance of the mean\n",
    "def mean_and_variance(weights, n):\n",
    "    mean = jnp.sum(weights, axis = -1) / n\n",
    "    variance = jnp.sum(weights**2, axis = -1) / n**2 - mean**2 / n\n",
    "    return mean, variance\n",
    "\n",
    "# lazy ln(mean) and variance of ln(mean)\n",
    "def ln_mean_and_variance(weights, n):\n",
    "    mean, variance = mean_and_variance(weights, n)\n",
    "    return jnp.log(mean), variance / mean**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46948f15-411d-4f68-8a98-3265be4c0407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_likelihood_and_variance(posteriors, injections, density, parameters):\n",
    "    pe_weights = density(posteriors, parameters) / posteriors['prior']\n",
    "    vt_weights = density(injections, parameters) / injections['prior']\n",
    "    num_obs, num_pe = pe_weights.shape\n",
    "    ln_lkls, pe_variances = ln_mean_and_variance(pe_weights, num_pe)\n",
    "    ln_pdet, vt_variance = ln_mean_and_variance(vt_weights, injections['total'])\n",
    "    ln_lkl = ln_lkls.sum() - ln_pdet * num_obs\n",
    "    variance = pe_variances.sum() + vt_variance * num_obs**2\n",
    "    # ln_lkl = jnp.nan_to_num(ln_lkl, nan = -jnp.inf)\n",
    "    # variance = jnp.nan_to_num(variance, nan = jnp.inf)\n",
    "    return ln_lkl, variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71b3d01-eaed-4b84-91a3-baff0c703bc1",
   "metadata": {},
   "source": [
    "#### Normalizing flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6904f4a1-2d11-4128-b338-bd740edd7117",
   "metadata": {},
   "source": [
    "Now let's set up the model that we'll train. We'll use a [block neural autoregressive flow](https://arxiv.org/abs/1904.04676) to approximate the population posterior. There's a nice library called [flowjax](https://github.com/danielward27/flowjax) to do normalizing flows in JAX that we'll use. This is built on top of [equinox](https://github.com/patrick-kidger/equinox), which will handle our neural networks.\n",
    "\n",
    "If you aren't familiar with normalizing flows, the (very brief) idea is that you can construct a probability distribution by transforming a simple known distribution (such as a standard normal distribution) with an invertible and differentiable function using the change-of-variables formula. For normalizing flows, that function is parametrized by a neural network, which is what makes the transformation flexible.\n",
    "\n",
    "The transformation is trained so that the output distribution best matches some target distribution - in our case the population posterior distribution. The variables being transformed are the population parameters above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7c1835-f135-4326-b0da-c0585e13e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox\n",
    "from flowjax.distributions import StandardNormal\n",
    "from flowjax.flows import block_neural_autoregressive_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15863a35-02f8-4b81-87d1-2e6d29891d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_init = block_neural_autoregressive_flow(\n",
    "    key = jax.random.key(0),\n",
    "    base_dist = StandardNormal(shape = (dim,)),\n",
    "    invert = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c011ea-9599-47ca-aab8-b27186d8fed3",
   "metadata": {},
   "source": [
    "We should take care that our normalizing flow is defined on the parameter domain we want it to be. In particular, our priors impose bounds on the range of values that can be taken. Therefore, we'll add some additional transformations to ensure those bounds are respected. These transformation are fixed and not trainable, unlike the flow transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0b4cab-2f98-4787-859b-82ade017eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flowjax.bijections import Affine, Sigmoid, Chain, Stack\n",
    "from flowjax.distributions import Transformed\n",
    "import paramax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba73f475-46f9-4a7a-a17c-e2e978ed26d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lo = jnp.array([priors[k].low for k in priors])\n",
    "hi = jnp.array([priors[k].high for k in priors])\n",
    "bijection = Chain([Sigmoid(shape = (dim,)), Affine(loc = lo, scale = hi - lo)])\n",
    "flow_init = Transformed(flow_init, paramax.non_trainable(bijection))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e01d02-1955-4c6d-987f-6e518a82aee2",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3e2d6a-caf3-44b5-a632-f14e9885cbb4",
   "metadata": {},
   "source": [
    "To train the flow, we need to define a loss function to minimize with respect to the neural-network parameters. For variational inference, the most common choice is the Kullback-Leibler divergence from the target posterior $\\mathcal{P}(\\Lambda)$ to the normalizing flow approximation $\\mathcal{Q}(\\Lambda)$, with $\\Lambda$ being the parameters of our population model that we want to infer:\n",
    "\n",
    "$$\n",
    "\\mathrm{KL}[\\mathcal{Q},\\mathcal{P}] = \\int \\mathrm{d}\\,\\Lambda\\, \\mathcal{Q}(\\Lambda) \\ln \\frac{\\mathcal{Q}(\\Lambda)}{\\mathcal{P}(\\Lambda)} .\n",
    "$$\n",
    "\n",
    "We know that our target posterior can be written using Bayes' theorem:\n",
    "\n",
    "$$\n",
    "\\mathcal{P}(\\Lambda) = \\frac { \\mathcal{L}(\\Lambda) \\pi(\\Lambda) } { \\mathcal{Z} } ,\n",
    "$$\n",
    "\n",
    "where $\\mathcal{L}(\\Lambda)$ is the likelihood function, $\\pi(\\Lambda)$ is the prior, and $\\mathcal{Z} = \\int \\mathrm{d}\\,\\Lambda\\, \\mathcal{L}(\\Lambda) \\pi(\\Lambda)$ is the evidence.\n",
    "\n",
    "We know the likelihood and prior - they're above - but we don't know the evidence. Therefore, the equivalent loss function is used:\n",
    "\n",
    "$$\n",
    "L = \\mathrm{KL}[\\mathcal{Q},\\mathcal{P}] - \\ln\\mathcal{Z} = \\int \\mathrm{d}\\,\\Lambda\\, \\mathcal{Q}(\\Lambda) \\ln \\frac{ \\mathcal{Q}(\\Lambda) }{ \\mathcal{L}(\\Lambda) \\pi(\\Lambda) } .\n",
    "$$\n",
    "\n",
    "This can be approximate with Monte Carlo integration using a batch of $M$ samples $\\{\\Lambda_i\\}_{i=1}^M$ drawn from the normalizing flow $\\mathcal{Q}$:\n",
    "\n",
    "$$\n",
    "L \\approx \\frac{1}{M} \\sum_{i=1}^M \\ln \\frac{ \\mathcal{Q}(\\Lambda_i) }{ \\mathcal{L}(\\Lambda_i) \\pi(\\Lambda_i) } .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6a876-4993-4072-9de2-b16b3d2d2952",
   "metadata": {},
   "source": [
    "First, let's choose some training settings. We'll use [optax](https://github.com/google-deepmind/optax) to optimize the neural-network parameters and [jax_tqdm](https://github.com/jeremiecoullon/jax-tqdm) to add a progress bar to our loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4d7146-9f96-4402-8d9e-f0b76f0c776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "import jax_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b658605c-30c5-4f49-ac53-52564adc35d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1 # perhaps surprisingly, this is sufficient\n",
    "steps = 10_000\n",
    "learning_rate = 1e-2\n",
    "optimizer = optax.adam(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a7c37-03d9-4968-af32-f56f4ad0a652",
   "metadata": {},
   "source": [
    "Now the loss function and training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb407410-d791-4609-91f1-402f5fe4acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the flow intro trainable and non-trainable partitions\n",
    "params_init, static = equinox.partition(\n",
    "    pytree = flow_init,\n",
    "    filter_spec = equinox.is_inexact_array,\n",
    "    is_leaf = lambda leaf: isinstance(leaf, paramax.NonTrainable),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d810bd51-28ca-4f9c-ba26-0aad895dc3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params, key):\n",
    "    flow = equinox.combine(params, static)\n",
    "    samples, ln_flows = flow.sample_and_log_prob(key, (batch_size,))\n",
    "    parameters = dict(zip(priors, samples.T))\n",
    "    single = lambda parameters: ln_likelihood_and_variance(\n",
    "        posteriors, injections, density, parameters,\n",
    "    )\n",
    "    ln_lkls, variances = jax.vmap(single)(parameters)\n",
    "    ln_priors = jnp.array(\n",
    "        [priors[k].log_prob(parameters[k]) for k in priors],\n",
    "    ).sum(axis = 0)\n",
    "    return jnp.mean(ln_flows - ln_priors - ln_lkls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b1a90f-5c91-4320-b2f7-24653794cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax_tqdm.scan_tqdm(steps, print_rate = 100, tqdm_type = 'std')\n",
    "def update(carry, step):\n",
    "    key, params, state = carry\n",
    "    key, subkey = jax.random.split(key)\n",
    "    loss, grad = equinox.filter_value_and_grad(loss_fn)(params, subkey)\n",
    "    updates, state = optimizer.update(grad, state, params)\n",
    "    params = equinox.apply_updates(params, updates)\n",
    "    carry = key, params, state\n",
    "    return carry, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf2dd5-f0bc-4fb0-b281-7b65bc7f44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22f456-e270-4b3e-80a9-f49d661ad667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, the training loop.\n",
    "# Sometimes the initial JIT compilation takes a few second to get going...\n",
    "\n",
    "state = optimizer.init(params_init)\n",
    "carry = jax.random.key(1), params_init, state\n",
    "\n",
    "t0 = time.time()\n",
    "carry, losses = jax.lax.scan(update, carry, jnp.arange(steps))\n",
    "dt = time.time() - t0\n",
    "print('total time including JIT compilation:', dt)\n",
    "\n",
    "key, params, state = carry\n",
    "flow = equinox.combine(params, static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab00f9-f804-4c22-b1ad-d6cd3365b634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss function values over training steps\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d63efe9-e18f-4535-9297-a6e69a46795e",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed36c1b1-05ff-4093-988e-b9c1e3b493b3",
   "metadata": {},
   "source": [
    "Now that the flow is trained, we can draw as many posterior samples as we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de8063-2fd4-4b48-bb56-9531aafd1318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corner import corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec47c9-aa00-4ea6-b884-655264303eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = flow.sample(jax.random.key(2), (10_000,))\n",
    "posterior = dict(zip(priors, samples.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f97ce4-406f-4313-b781-3a57f1e9ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "corner(np.array(samples), labels = list(priors));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a420b772-e2fa-441a-a76b-a0a516a6bbf7",
   "metadata": {},
   "source": [
    "Let's also plot the inferred population-level distributions of source parameters and their posterior uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e0f1d0-e332-4301-a0d8-777aad91f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = dict(\n",
    "    mass_1_source = jnp.linspace(2, 100, 1_000),\n",
    "    mass_ratio = jnp.linspace(0, 1, 1_000),\n",
    "    a = jnp.linspace(0, 1, 1_000),\n",
    "    cos_tilt = jnp.linspace(-1, 1, 1_000),\n",
    "    redshift = jnp.linspace(0, 2, 1_000),\n",
    ")\n",
    "\n",
    "# the mass ratio model is conditional on the primary mass, so we have to marginalize\n",
    "def pdf_q_marginal(q, parameters):\n",
    "    x, y = jnp.meshgrid(q, grid['mass_1_source'], indexing = 'ij')\n",
    "    p = pdf_q_given_m(x, y, parameters) * pdf_m(y, parameters)\n",
    "    return jnp.trapezoid(p, y, axis = 1)\n",
    "\n",
    "pdf = dict(\n",
    "    mass_1_source = pdf_m,\n",
    "    mass_ratio = pdf_q_marginal,\n",
    "    a = pdf_a,\n",
    "    cos_tilt = pdf_c,\n",
    "    redshift = pdf_z,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7591a78a-94a0-4286-9d7d-e36b77d8684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(k, data):\n",
    "    # we use sequential map for mass ratio because the integral uses more memory\n",
    "    single = lambda parameters: pdf[k](grid[k], parameters)\n",
    "    if k == 'mass_ratio':\n",
    "        ps = jax.lax.map(single, data)\n",
    "    else:\n",
    "        ps = jax.vmap(single)(data)\n",
    "\n",
    "    for qs, alpha in (\n",
    "        ((0.005, 0.995), 0.2),\n",
    "        ((0.05, 0.95), 0.3),\n",
    "        ((0.25, 0.75), 0.4),\n",
    "    ):\n",
    "        label = f'{(qs[1]-qs[0]) * 100:.0f}% posterior'\n",
    "        plt.fill_between(\n",
    "            grid[k], *np.quantile(ps, qs, axis = 0), label = label,\n",
    "            color = 'C0', alpha = alpha, lw = 0,\n",
    "        )\n",
    "\n",
    "    plt.plot(\n",
    "        grid[k], np.median(ps, axis = 0), label = 'median posterior',\n",
    "        c = 'C1', lw = 2,\n",
    "    )\n",
    "    plt.plot(\n",
    "        grid[k], np.mean(ps, axis = 0), label = 'mean posterior (PPD)',\n",
    "        c = 'C2', lw = 2, ls = '--',\n",
    "    )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(k)\n",
    "    plt.ylabel(f'p({k})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42cda2a-47a9-453b-bdc2-894540ec4e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in 'mass_1_source', 'mass_ratio', 'a', 'cos_tilt', 'redshift':\n",
    "    make_plot(k, posterior)\n",
    "\n",
    "    if k == 'mass_1_source':\n",
    "        plt.semilogy()\n",
    "        plt.ylim(1e-5, 1e0)\n",
    "    elif k == 'mass_ratio':\n",
    "        plt.semilogy()\n",
    "        plt.ylim(1e-2, 1e1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb18011-d3fa-4e2a-88d7-21bdd4da5f00",
   "metadata": {},
   "source": [
    "#### Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16da4c9b-8aa9-4bde-8840-57c4656eeac4",
   "metadata": {},
   "source": [
    "There is some immediate tinkering you can do with the code above:\n",
    "- The training settings, e.g., batch size, number of training steps, learning rate, optimizer etc. Try a [learning-rate scheduler](https://optax.readthedocs.io/en/latest/api/optimizer_schedules.html), for example.\n",
    "- The [flow settings](https://danielward27.github.io/flowjax/api/flows.html#flowjax.flows.block_neural_autoregressive_flow), e.g., try making the network smaller or larger.\n",
    "- The flow itself, i.e., try [a different type](https://danielward27.github.io/flowjax/api/flows.html) of normalizing flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71caa29d-d355-4f7f-8183-c65ff3086ddd",
   "metadata": {},
   "source": [
    "Try targeting a different gravitational-wave population:\n",
    "- Try altering some of the population models.\n",
    "- Try a completely different population models that you're interested in testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09dedda-1aed-4a13-98b3-f25f9715c270",
   "metadata": {},
   "source": [
    "You could compare to different code backends or packages:\n",
    "- Try implementing this in your favourite ML package, e.g., PyTorch, TensorFlow, Julia, etc.\n",
    "- Compare the results here to an existing variational inference package, e.g., [pyro](https://docs.pyro.ai/en/stable/inference_algos.html) or [numpyro](https://num.pyro.ai/en/latest/svi.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a681af54-e328-47fc-8dc4-97a49b609776",
   "metadata": {},
   "source": [
    "We should check our results:\n",
    "- Do you trust the posterior predicted by the flow and how would you test it?\n",
    "- How could you use the normalizing flow to compute the Bayesian evidence for model comparison?\n",
    "- What about the Monte Carlo variance - is it under control?\n",
    "- Try reusing the likelihood function we coded up with a stochastic sampling algorithm to compare posteriors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be1ead-c594-448c-b117-c9908aadc5ac",
   "metadata": {},
   "source": [
    "In https://arxiv.org/abs/2504.07197, we have several tips for training and inference validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2087bd91-06ca-4040-be3d-fc5e4a319f18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
