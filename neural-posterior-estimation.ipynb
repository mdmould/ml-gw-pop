{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6034bd4-d6ef-4e23-968f-e1b4f818ad53",
   "metadata": {},
   "source": [
    "If you're running in a separate notebook (e.g., Google Colab), go through and un-comment the cells below as required. Also make sure to set the runtime before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df86935-74a6-4ecb-a578-4c985c4d7809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you're running on a shared cluster and want to limit the resources you take up:\n",
    "# import os\n",
    "# os.environ[\"OPENBLAS_NUM_THREADS\"] = '1'\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = '1'\n",
    "# os.environ[\"VECLIB_MAXIMUM_THREADS\"] = '1'\n",
    "# os.environ[\"NUMEXPR_NUM_THREADS\"] = '1'\n",
    "# os.environ['OMP_NUM_THREADS'] = '1'\n",
    "# os.environ['NPROC'] = '1'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0' # you can change to a GPU ID not in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da3ca16-2939-460a-b5c4-ce3fe1577b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy matplotlib corner h5ify\n",
    "# !pip install wcosmo jax_tqdm equinox equinox optax flowjax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a9f787-5cdd-420d-ab58-bfe03917ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you're running on CPU:\n",
    "# !pip install jax numypro\n",
    "\n",
    "# # If you're running on GPU\n",
    "# !pip install -U 'jax[cuda12]'\n",
    "# !pip install 'numpyro[cuda]' -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2f7cd2-8b21-49a8-9638-2b4677d91e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download software injections from LVK O3:\n",
    "# !mkdir -p data\n",
    "# !wget https://github.com/mdmould/ml-gw-pop/raw/refs/heads/main/data/vt.h5 -P data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad3fd0e-bbb3-49c5-a2ba-390519def43a",
   "metadata": {},
   "source": [
    "## Neural posterior estimation for gravitational-wave population inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950a1e2c-dc43-4ace-a99e-93ba1129ea48",
   "metadata": {},
   "source": [
    "In this notebook, we'll train a normalizing flow to learn the Bayesian posterior for an astrophysical population model from gravitational-wave catalogues using simulation-based inference. This will be a toy pedagogical example, but hopefully you can it as a base to build a more realistic inference pipeline. We'll focus on just binary black-hole mergers. We'll use [JAX](https://github.com/jax-ml/jax) as the main workhorse behind this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dec4fd-a6b5-4f22-bff6-87c72a06f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "jax.config.update('jax_enable_x64', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e2878c-cb1f-4aac-b642-99e73e529b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for GPU devices\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143d04fc-3f10-4534-8bcc-abff2c5e69ab",
   "metadata": {},
   "source": [
    "#### Population model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dce68ca-31f0-47d3-8598-7ba9f19c3e93",
   "metadata": {},
   "source": [
    "First, let's define the population model that we'll use to model the astrophysical distribution of sources. We'll only include source-frame primary masses, binary mass ratio, and merger redshift in our model.\n",
    "\n",
    "- The primary masses will follow a simplified version of the [Power Law + Peak](https://arxiv.org/abs/1801.02699) model. It's parameters are:\n",
    "  - alpha = power law slope,\n",
    "  - mu = location of peak,\n",
    "  - sigma = width of peak,\n",
    "  - f = fraction of source in peak,\n",
    "  - mmin = minimum primary mass,\n",
    "  - mmax = maximum mass.\n",
    "- The mass ratios will follow a simple power law. Parameters:\n",
    "  - beta = power law slope.\n",
    "- We'll assume that the merger rate evolves over comoving volume and source-frame time as a [power law in redshift](https://arxiv.org/abs/1805.10270). Parameters:\n",
    "  - gamma = power law slope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60bc62f-0dff-4345-912e-c7bdfc81cc3f",
   "metadata": {},
   "source": [
    "We'll use [wcosmo](https://github.com/ColmTalbot/wcosmo), which is a nice package for cosmological calculations in JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2411539e-3170-4fdb-8517-69d104bb9429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wcosmo\n",
    "wcosmo.disable_units()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aefa58-2f03-4853-a30a-1474b3cab6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_powerlaw(x, alpha, xmin, xmax):\n",
    "    cut = (xmin <= x) * (x <= xmax)\n",
    "    shape = x**alpha\n",
    "    norm = (xmax**(alpha + 1) - xmin**(alpha+1)) / (alpha + 1)\n",
    "    return cut * shape / norm\n",
    "\n",
    "def truncated_normal(x, mu, sigma, xmin, xmax):\n",
    "    cut = (xmin <= x) * (x <= xmax)\n",
    "    shape = jax.scipy.stats.norm.pdf(x, mu, sigma)\n",
    "    norm = (\n",
    "        - jax.scipy.stats.norm.cdf(xmin, mu, sigma)\n",
    "        + jax.scipy.stats.norm.cdf(xmax, mu, sigma)\n",
    "    )\n",
    "    return cut * shape / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24dbf83-43af-45a2-a6ff-b7f6fc402ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_mass_1_source(x, parameters):\n",
    "    mmin, mmax = 2, 100\n",
    "    pl = truncated_powerlaw(x, parameters['alpha'], mmin, mmax)\n",
    "    tn = truncated_normal(x, parameters['mu'], parameters['sigma'], mmin, mmax)\n",
    "    return (1 - parameters['f']) * pl + parameters['f'] * tn\n",
    "\n",
    "def pdf_mass_ratio(x, parameters):\n",
    "    q_min, q_max = 0.01, 1\n",
    "    return truncated_powerlaw(x, parameters['beta'], q_min, q_max)\n",
    "\n",
    "def shape_redshift(x, parameters):\n",
    "    return (1 + x)**parameters['gamma']\n",
    "\n",
    "def pdf_redshift(x, parameters):\n",
    "    zmax = 2\n",
    "    fn = lambda z: (\n",
    "        shape_redshift(z, parameters)\n",
    "        * wcosmo.Planck15.differential_comoving_volume(z) * 4 * jnp.pi / 1e9\n",
    "    )\n",
    "    cut = (0 < x) * (x <= zmax)\n",
    "    shape = fn(x)\n",
    "    zz = jnp.linspace(0, zmax, 10_000)\n",
    "    norm = jnp.trapezoid(fn(zz), zz)\n",
    "    return cut * shape / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa9f151-a775-4b7d-b72d-9c57e4c238d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def density(data, parameters):\n",
    "    return (\n",
    "        pdf_mass_1_source(data['mass_1_source'], parameters)\n",
    "        * pdf_mass_ratio(data['mass_ratio'], parameters)\n",
    "        * pdf_redshift(data['redshift'], parameters)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f91fe4a-6ac2-480d-a4e6-057999a77dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of source parameters we include\n",
    "dim_event = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d9eb8-fc65-4787-962f-2c9572cb98d2",
   "metadata": {},
   "source": [
    "Let's plot what the population models look like for some parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c81980-e27c-4d36-b663-8b8c78c6d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3333cd1d-df0c-47e2-a654-2b3d8d30e4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict(\n",
    "    alpha = -3,\n",
    "    mu = 35,\n",
    "    sigma = 3,\n",
    "    f = 0.05,\n",
    "    beta = 1,\n",
    "    gamma = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c737b6-4eb4-44d3-b590-7d62e3c0629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = jnp.linspace(2, 100, 1_000)\n",
    "p = pdf_mass_1_source(m, parameters)\n",
    "\n",
    "plt.plot(m, p)\n",
    "plt.semilogy()\n",
    "plt.xlabel('primary mass')\n",
    "plt.ylabel('PDF')\n",
    "\n",
    "print(jnp.trapezoid(p, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c694421b-d0de-46fb-965d-8ef7e7cfd82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = jnp.linspace(0, 1, 1_000)\n",
    "p = pdf_mass_ratio(q, parameters)\n",
    "\n",
    "plt.plot(q, p)\n",
    "plt.xlabel('mass ratio')\n",
    "plt.ylabel('PDF')\n",
    "\n",
    "print(jnp.trapezoid(p, q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff611a91-202e-4cff-8b52-25d49e280a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = jnp.linspace(0, 2, 1_000)\n",
    "p = pdf_redshift(z, parameters)\n",
    "\n",
    "plt.plot(z, p)\n",
    "plt.xlabel('redshift')\n",
    "plt.ylabel('evolution over redshift')\n",
    "\n",
    "print(jnp.trapezoid(p, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078a7bef-6971-4eb0-9887-99980fd79e8b",
   "metadata": {},
   "source": [
    "Next, we'll set priors on the parameters of the population model - these are the parameters we want to measure from data. The priors will be the distributions that we draw from to train the model and also correspond to the Bayesian posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e06cc9-61ae-43da-9152-a4c7d7683064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca42d430-edcc-4fcd-ad0c-db98ba79f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = dict(\n",
    "    alpha = numpyro.distributions.Uniform(-5, 0),\n",
    "    mu = numpyro.distributions.Uniform(20, 50),\n",
    "    sigma = numpyro.distributions.Uniform(1, 10),\n",
    "    f = numpyro.distributions.Uniform(0, 0.2),\n",
    "    beta = numpyro.distributions.Uniform(0, 5),\n",
    "    gamma = numpyro.distributions.Uniform(-5, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d9c686-72dd-43eb-b90b-50dac639b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of population parameters\n",
    "dim_pop = len(priors)\n",
    "dim_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25302b2-5b26-42b2-9052-85dbaa63ad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function makes a single draw from these priors.\n",
    "def sample_parameters(key):\n",
    "    keys = jax.random.split(key, len(priors))\n",
    "    return {k: priors[k].sample(key) for k, key in zip(priors, keys)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470edaf0-a41c-432d-9bac-ef5510e72f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_parameters(jax.random.key(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5714fe1-bb9f-4c9c-8d1f-64cbb5a996a2",
   "metadata": {},
   "source": [
    "#### Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4744dc89-a74c-4ea2-9ca0-02677541056e",
   "metadata": {},
   "source": [
    "Now we need some fake observations to train the model with. The way we generate data should follow our model for how the observed catalogue is actually produced. This proceeds as follows:\n",
    "\n",
    "1. Fix a realization of the universe with population parameters $\\Lambda$ (the parameters above).\n",
    "2. Draw a source with parameters $\\theta$ (primary mass, mass ratio, redshift) from the population model.\n",
    "3. Generate a gravitational-wave signal $s=h(\\theta)$ using a waveform model $h$ and add it to detector noise $n$ to produce data $d=n+s$.\n",
    "4. Decide whether the signal in data $d$ is detected (\"det\") or not.\n",
    "5. Repeat 2-4 for many for many observations over an observing run.\n",
    "\n",
    "To infer population parameters $\\Lambda$ from data $d$, we need to invert this forward model. The inverse is given by the joint distribution\n",
    "$$\n",
    "p(\\mathrm{det},d,\\theta,\\Lambda) = p(\\mathrm{det}|d) p(d|\\theta) p(\\theta|\\Lambda) p(\\Lambda) .\n",
    "$$\n",
    "This is the probabilistic model that we'll try to get a normalizing flow to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1046f346-88eb-488e-9f25-b250b3e7a133",
   "metadata": {},
   "source": [
    "Here's where we'll cheat a bit by reusing the public binary black-hole software injections provided by the LVK from O3. Our \"observations\" will be the values of the source parameters (primary mass, mass ratio, and redshift) of injections that were classed as detected. Of course, this is not a realistic setting as we do not observe the source parameters directly, but will serve the purpose for this notebook.\n",
    "\n",
    "The injections are pre-prepared in the file below (the scripts in the `data/` folder were used to download and prepare the data), which contains the values of the source parameters as well as the corresponding probability densities of the distribution from which they were drawn (\"prior\"), which we'll call $q(\\theta)$ and is defined [here](https://zenodo.org/records/7890437)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2a1e49-42d7-4d54-83fc-b0b0e28efe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5ify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159487b1-c1fc-463b-94e7-2ca53c69eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "injections = h5ify.load('data/vt.h5')\n",
    "injections = {\n",
    "    k: jnp.array(injections[k], dtype = jnp.float64).squeeze()\n",
    "    for k in ('mass_1_source', 'mass_ratio', 'redshift', 'prior')\n",
    "}\n",
    "\n",
    "injections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715fbc81-5790-4d46-be49-65e873f65a62",
   "metadata": {},
   "source": [
    "However, in the generative model above, we want to draw sources from $p(\\theta|\\Lambda)$, not $q(\\theta)$. So we'll play another trick:\n",
    "$$\n",
    "p(\\mathrm{det},d,\\theta,\\Lambda) = p(\\mathrm{det}|d) p(d|\\theta) \\frac{p(\\theta|\\Lambda)}{q(\\theta)} q(\\theta) p(\\Lambda) .\n",
    "$$\n",
    "In words, since all of the sources in the above file are detected (\"det\") by definition, we can draw detections from $p(\\theta|\\Lambda)$ by instead drawing detections from $q(\\theta)$ according to draw probabilities $\\propto p(\\theta|\\Lambda) / q(\\theta)$ for given population parameters $\\Lambda$.\n",
    "\n",
    "This requires $\\mathrm{supp}\\,p(\\theta|\\Lambda) \\subset \\mathrm{supp}\\,q(\\theta)$, which I conveniently chose the priors on $\\Lambda$ above to make sure is true. However, using these injections means we'll have a finite amount of training data. And when $p(\\theta|\\Lambda)$ is very different from $q(\\theta)$, we might reuse the same training data many times.\n",
    "\n",
    "Below we define a function to draw a catalogue of detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab37f6-1e77-4c80-b575-de1774084154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_detections(key, num_obs, parameters):\n",
    "    weights = density(injections, parameters) / injections['prior']\n",
    "    idxs = jax.random.choice(\n",
    "        key, weights.size, shape = (num_obs,), p = weights,\n",
    "    )\n",
    "    return {k: injections[k][idxs] for k in injections}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a9dcc-952a-49eb-a31e-84d149b02e40",
   "metadata": {},
   "source": [
    "We can see how selection effects alter the underlying distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149cecdd-1383-4eef-b78d-1fb8ad639a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict(\n",
    "    alpha = -3,\n",
    "    mu = 35,\n",
    "    sigma = 3,\n",
    "    f = 0.05,\n",
    "    beta = 1,\n",
    "    gamma = 0,\n",
    ")\n",
    "\n",
    "detections = sample_detections(jax.random.key(1), 1_000, parameters)\n",
    "plt.hist(\n",
    "    detections['mass_1_source'], density = True, bins = 30,\n",
    "    label = 'detections',\n",
    ")\n",
    "\n",
    "m = jnp.linspace(2, 100, 1_000)\n",
    "p = pdf_mass_1_source(m, parameters)\n",
    "plt.plot(m, p, label = 'underlying population')\n",
    "\n",
    "plt.legend()\n",
    "plt.semilogy()\n",
    "plt.xlabel('primary mass')\n",
    "plt.ylabel('PDF');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dde968-bb5c-4f62-9fd6-fe07c47d3296",
   "metadata": {},
   "source": [
    "#### Normalizing flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2704bfcb-f9ad-418a-b621-7729040d7cb7",
   "metadata": {},
   "source": [
    "Now let's set up the model that we'll train. We'll use a [block neural autoregressive flow](https://arxiv.org/abs/1904.04676) to approximate the population posterior. There's a nice library called [flowjax](https://github.com/danielward27/flowjax) to do normalizing flows in JAX that we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b51e20f-8bb3-469d-9e50-1de9614b13ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flowjax.distributions import StandardNormal\n",
    "from flowjax.flows import block_neural_autoregressive_flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef3d305-ce41-4e2a-93c2-882b069f1479",
   "metadata": {},
   "source": [
    "If you aren't familiar with normalizing flows, the (very brief) idea is that you can construct a probability distribution by transforming a simple known distribution (such as a standard normal distribution) with an invertible and differentiable function using the change-of-variables formula. For normalizing flows, that function is parametrized by a neural network, which is what makes the transformation flexible.\n",
    "\n",
    "The transformation is trained so that the output distribution best matches some target distribution - in our case the posterior distribution of the parameters of the population model. The variables being transformed are the population parameters $\\Lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912e2fc7-d63a-4d12-a74f-54f90432a3c9",
   "metadata": {},
   "source": [
    "Importantly, normalizing flows can model conditional probability distributions by making the transformation depend on some additional inputs - for posterior distributions, that is the observational data. In our case, we would like to condition on a catalogue of observations. We'll fix the number of observations in the catalogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c08066-8d3f-41b0-9509-35cb6627f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the fixed number of observations in the catalogue\n",
    "num_obs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e31caba-093f-47fc-ac17-1d01979ffb31",
   "metadata": {},
   "source": [
    "For complicated data, it is typical to produce an embedded repesentation of it to more efficiently extract the information it contains, e.g., with another neural network; here, we'll keep things simple and just stack \"observations\" together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2298c8-981f-460d-9f63-864448754203",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_init = block_neural_autoregressive_flow(\n",
    "    key = jax.random.key(2),\n",
    "    base_dist = StandardNormal(shape = (dim_pop,)),\n",
    "    cond_dim = num_obs * dim_event,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15edef3-b8c9-4c9e-87b8-8400578a7a7f",
   "metadata": {},
   "source": [
    "We should take care that our normalizing flow is defined on the parameter domain we want it to be. In particular, our priors on $\\Lambda$ impose bounds on the range of values that can be taken. Therefore, we'll add some additional transformations to ensure those bounds are respected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d9d812-a941-4b41-ab97-822b3d4258c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations for population parameters\n",
    "\n",
    "lo_pop = jnp.array([priors[k].low for k in priors])\n",
    "hi_pop = jnp.array([priors[k].high for k in priors])\n",
    "\n",
    "# map from unconstrained space to constrained space\n",
    "def forward_parameters(x):\n",
    "    y = jax.scipy.stats.norm.cdf(x) * (hi_pop - lo_pop) + lo_pop\n",
    "    parameters = dict(zip(priors, y))\n",
    "    return parameters\n",
    "\n",
    "def inverse_parameters(parameters):\n",
    "    x = jnp.array([parameters[k] for k in priors])\n",
    "    x = jax.scipy.stats.norm.ppf((x - lo_pop) / (hi_pop - lo_pop))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a7e95-2d4f-4d94-8ec1-974be173e2cd",
   "metadata": {},
   "source": [
    "Though not strictly necessary, we'll also do this for the observations so that all inputs to the flow have roughly the same range of numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1673a7e5-7af7-4136-b496-eefa77c21153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformtions for source parameters\n",
    "\n",
    "# primary mass, mass ratio, redshift\n",
    "lo_event = jnp.array([2, 0.01, 0])\n",
    "hi_event = jnp.array([100, 1, 2])\n",
    "\n",
    "x = jnp.array([\n",
    "    injections['mass_1_source'],\n",
    "    injections['mass_ratio'],\n",
    "    injections['redshift'],\n",
    "]).T\n",
    "x = (x - lo_event) / (hi_event - lo_event)\n",
    "x = jax.scipy.stats.norm.ppf(x)\n",
    "loc = x.mean(axis = 0)\n",
    "scale = x.std(axis = 0)\n",
    "x = (x - loc) / scale\n",
    "\n",
    "# we only need the inverse to embed the data into the flow\n",
    "def inverse_detections(detections):\n",
    "    x = jnp.array(\n",
    "        [detections[k] for k in ('mass_1_source', 'mass_ratio', 'redshift')]\n",
    "    ).T # shape = (num_obs, dim_event)\n",
    "    x = (x - lo_event) / (hi_event - lo_event)\n",
    "    x = jax.scipy.stats.norm.ppf(x)\n",
    "    x = (x - loc) / scale # shape = (num_obs, dim_event)\n",
    "    x = x.ravel() # shape = (num_obs * dim_event,)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43ad8e9-1deb-4151-8adf-d405ed460d26",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df9d22-71a8-4fd6-b8a8-8da8860f7733",
   "metadata": {},
   "source": [
    "To train the flow, we need to define a loss function to minimize with respect to the neural-network parameters. The training objective for flows can be thought of in several equivalent ways, including the Kullback-Leibler divergence, the cross entropy, and the likelihood of the training data.\n",
    "\n",
    "The upshot is that the probability density predicted by the flow can be matched to the joint distribution we defined in [Training data](#Training-data) - we never need to compare it against the actual posterior distribution, which is the key insight that first enabled neural posterior estimation.\n",
    "\n",
    "Below, we define the loss function and training loop to do this. Note that selection effects are automatically included in the generative model and thus the flow posterior (one of the reasons that simulation-based inference is nice in this context)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fdbe0c-5955-4a88-9744-2a0a662c6f52",
   "metadata": {},
   "source": [
    "First, let's set some training parameters. We'll use [optax](https://github.com/google-deepmind/optax) to update the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7eac46-a5d3-477f-a98f-a5caceee72df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dcd7e5-9656-485e-afea-e4bb579b9314",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "steps = 10_000\n",
    "learning_rate = 1e-2\n",
    "optimizer = optax.adam(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce99adb-0f28-4770-9cf8-b222ac4ac8ba",
   "metadata": {},
   "source": [
    "flowjax is built on top of [equinox](https://github.com/patrick-kidger/equinox), which (among many other cool things it does) manages the neural network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164be139-36d8-4306-9917-b7f1bfbf95fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c428f0-ab3c-40b9-a9d7-24768d3861c1",
   "metadata": {},
   "source": [
    "It works by splitting pytrees - nested python containers of JAX arrays - into trainable and non-trainable subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dda2a30-7e18-49c1-82c0-f82fdda31153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, trainable parameters are any floating-point arrays:\n",
    "params_init, static = equinox.partition(flow_init, equinox.is_inexact_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae25b99-56d8-4391-8a1a-2adbcb2e7f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to sample a single training example:\n",
    "# - a single set of population parameters\n",
    "# - a catalogue of num_obs detections\n",
    "def sample(key):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    parameters = sample_parameters(subkey)\n",
    "    x = inverse_parameters(parameters)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    detections = sample_detections(subkey, num_obs, parameters)\n",
    "    c = inverse_detections(detections)\n",
    "    return x, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fee4d5d-f657-4f30-9171-d967cd9311b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss function\n",
    "# takes the mean over a batch of batch_size training examples\n",
    "def loss_fn(params, key):\n",
    "    keys = jax.random.split(key, batch_size)\n",
    "    xs, cs = jax.vmap(sample)(keys)\n",
    "    flow = equinox.combine(params, static) # rebuild the flow from partitions\n",
    "    return -flow.log_prob(xs, cs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d3e1f-856f-45a0-8092-e4b582dc8741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to update the neural-network parameters\n",
    "def update(carry, step):\n",
    "    key, params, state = carry\n",
    "    key, subkey = jax.random.split(key)\n",
    "    loss, grad = equinox.filter_value_and_grad(loss_fn)(params, subkey)\n",
    "    updates, state = optimizer.update(grad, state, params)\n",
    "    params = equinox.apply_updates(params, updates)\n",
    "    carry = key, params, state\n",
    "    return carry, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e00cbe9-468a-4498-bcb4-050624ed6ed8",
   "metadata": {},
   "source": [
    "Another handy package is [jax_tqdm](https://github.com/jeremiecoullon/jax-tqdm) to add progress bar to JAX loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fb88ab-3871-4370-bb91-62c7dea34213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c86a2-d5e5-4d9b-bf58-6b454d981eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "update = jax_tqdm.scan_tqdm(steps, print_rate = 100, tqdm_type = 'std')(update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a99de6f-7739-4d8c-a061-b99b333eaaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, the training loop (written without Python loops using JAX)\n",
    "\n",
    "state = optimizer.init(params_init)\n",
    "carry = jax.random.key(3), params_init, state\n",
    "\n",
    "carry, losses = jax.lax.scan(update, carry, jnp.arange(steps))\n",
    "key, params, state = carry\n",
    "\n",
    "flow = equinox.combine(params, static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589aa7c6-e596-4e3a-8026-34c0803178ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss function values over training steps\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341dd32a-ee71-4032-99a1-dd6ad59f08d1",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87221047-5ae5-4dde-97ab-a9a427fd8f35",
   "metadata": {},
   "source": [
    "Now that the model is trained, let's infer the popoulation posterior on a mock catalogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a4f264-10d7-402d-bd65-a45b8d641987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a mock catalogue\n",
    "\n",
    "parameters = dict(\n",
    "    alpha = -3,\n",
    "    mu = 35,\n",
    "    sigma = 3,\n",
    "    f = 0.05,\n",
    "    beta = 1,\n",
    "    gamma = 0,\n",
    ")\n",
    "\n",
    "key = jax.random.key(np.random.randint(1e9))\n",
    "detections = sample_detections(key, num_obs, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9b040a-150b-4bfd-b513-9c1740ea3e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw posterior samples from the flow\n",
    "\n",
    "c = inverse_detections(detections)\n",
    "x = flow.sample(jax.random.key(4), (10_000,), condition = c)\n",
    "posterior = jax.vmap(forward_parameters)(x)\n",
    "\n",
    "posterior.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8e747-0af6-4dc7-ac06-5a5d3da5850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corner import corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6517bac3-9a6d-4a8f-9914-cb58fa91c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "corner(\n",
    "    np.transpose([posterior[k] for k in priors]),\n",
    "    labels = list(priors),\n",
    "    truths = [parameters[k] for k in priors],\n",
    "    level = (0.5, 0.9, 0.99),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936491ae-c96f-4535-a944-f6d06e6e4660",
   "metadata": {},
   "source": [
    "Let's also plot the inferred population-level distributions of source parameters and their posterior uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084b4f81-0d4c-4e23-adb4-b5a0219aa502",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = dict(\n",
    "    mass_1_source = jnp.linspace(2, 100, 1_000),\n",
    "    mass_ratio = jnp.linspace(0, 1, 1_000),\n",
    "    redshift = jnp.linspace(0, 2, 1_000),\n",
    ")\n",
    "\n",
    "pdf = dict(\n",
    "    mass_1_source = pdf_mass_1_source,\n",
    "    mass_ratio = pdf_mass_ratio,\n",
    "    redshift = pdf_redshift,\n",
    ")\n",
    "\n",
    "def make_plot(k):\n",
    "    plt.hist(\n",
    "        detections[k], bins = 50, density = True, label = 'observed',\n",
    "        color = 'C0',\n",
    "    )\n",
    "\n",
    "    ps = jax.vmap(lambda parameters: pdf[k](grid[k], parameters))(posterior)\n",
    "    for qs, alpha in (\n",
    "        ((0.005, 0.995), 0.2),\n",
    "        ((0.05, 0.95), 0.4),\n",
    "        ((0.25, 0.75), 0.6),\n",
    "    ):\n",
    "        label = f'{(qs[1]-qs[0]) * 100:.0f}% posterior'\n",
    "        plt.fill_between(\n",
    "            grid[k], *np.quantile(ps, qs, axis = 0), label = label,\n",
    "            color = 'C1', alpha = alpha, lw = 0,\n",
    "        )\n",
    "\n",
    "    plt.plot(\n",
    "        grid[k], pdf[k](grid[k], parameters), label = 'true astrophysical',\n",
    "        c = 'C2',\n",
    "    )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(k)\n",
    "    plt.ylabel(f'p({k})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc406127-3c29-4e43-a6d8-c8289c5a1c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot('mass_1_source')\n",
    "plt.semilogy()\n",
    "plt.ylim(1e-6, 1e1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b44f020-4914-4c00-8ae0-3972e3b28497",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot('mass_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65379fe1-2ec6-47b3-b506-3fa8563e2b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot('redshift')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91910de5-1a76-4421-9fcd-4340b337b5bd",
   "metadata": {},
   "source": [
    "#### Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a2616-77e6-4c2e-8ded-578f341a6cb4",
   "metadata": {},
   "source": [
    "There is some immediate tinkering you can do with the code above:\n",
    "- The training settings, e.g., batch size, number of training steps, learning rate, optimizer etc. Try a [learning-rate scheduler](https://optax.readthedocs.io/en/latest/api/optimizer_schedules.html), for example.\n",
    "- The [flow settings](https://danielward27.github.io/flowjax/api/flows.html#flowjax.flows.block_neural_autoregressive_flow), e.g., try making the network smaller or larger.\n",
    "- The flow itself, i.e., try [a different type](https://danielward27.github.io/flowjax/api/flows.html) of normalizing flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce768c-bfd7-4ca2-916b-cea6986c05b7",
   "metadata": {},
   "source": [
    "Try targeting a different gravitational-wave population:\n",
    "- Change the catalogue size.\n",
    "- Add more parameters to the population model.\n",
    "- Change the population model to something more sophisticated.\n",
    "- Include additional source parameters, e.g., black-hole spins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510939e6-4758-4e26-b3b1-76fe33cb6af9",
   "metadata": {},
   "source": [
    "We should check our results:\n",
    "- Test that the model is properly converged.\n",
    "- Check that the model is not overfitting.\n",
    "- Compare flow predictions at different points along training, e.g., return the flow with the best loss (at the moment we just take the flow at the last training step).\n",
    "- How could we check that the flow-predicted posterior is correct?\n",
    "- How could we use separate distributions for the training prior and inference prior?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f935c9fe-9cce-4f9f-983e-88424133f903",
   "metadata": {},
   "source": [
    "Make everything more realistic:\n",
    "- We don't directly observe the source parameters, so what observational data should we input instead?\n",
    "- Can the model be applied to varying catalogue sizes?\n",
    "- How can we apply this for large catalogues from future gravitational-wave detectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59c744-5cd4-404f-b8a2-7980f15d898d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
