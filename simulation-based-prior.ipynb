{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05629f79-710d-42f1-a2ff-ba19ae032ab7",
   "metadata": {},
   "source": [
    "If you're running in a separate notebook (e.g., Google Colab), go through and un-comment the cells below as required. Also make sure to set the runtime before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee84ca38-867c-4a27-b469-7bb8f11ce30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy matplotlib corner h5ify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755fdc9a-fa7d-4ae3-83da-2e7f49b8c76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you're running on CPU:\n",
    "# !pip install jax numypro\n",
    "\n",
    "# # If you're running on GPU\n",
    "# !pip install -U 'jax[cuda12]'\n",
    "# !pip install 'numpyro[cuda]' -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19265c12-f294-4a88-82d8-55b9470f85db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wcosmo jax_tqdm equinox equinox optax flowjax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ef9666-6632-41e5-b3ae-296df3530963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download software injections and parameter estimation from LVK O3:\n",
    "# !mkdir -p data\n",
    "# !wget https://github.com/mdmould/ml-gw-pop/blob/main/data/vt.h5 -P data\n",
    "# !wget https://github.com/mdmould/ml-gw-pop/blob/main/data/pe.h5 -P data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef958a-7187-4607-93a5-e025b1d5b5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're running on a shared cluster and want to limit the resources you take up:\n",
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = '1'\n",
    "os.environ[\"MKL_NUM_THREADS\"] = '1'\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = '1'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = '1'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['NPROC'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b7abc-c905-476e-9510-0eac7d81ad69",
   "metadata": {},
   "source": [
    "## Simulation-based population model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40221c43-7a01-4d01-a75a-739acfb5966c",
   "metadata": {},
   "source": [
    "In this notebook, we'll train a normalizing flow to learn an astrophysical population model using simulations and use it for population inference on a catalogue of gravitational-wave observations. We'll focus on just binary black-hole mergers. We'll use [JAX](https://github.com/jax-ml/jax) as the main workhorse behind this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba92f396-e8d9-4eff-aecd-d954c2e7e9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "jax.config.update('jax_enable_x64', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8872384d-c062-4889-9bdc-30841d4dcb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for GPU devices\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5021bc7b-3955-4984-b210-ba2dfc0f26d6",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d066b-920b-4bfd-9816-c96662a2907b",
   "metadata": {},
   "source": [
    "We will perform population inference on the catalogue of black-hole mergers with false-alarm rates > 1/year from O3. Below, we load in pre-prepared parameter estimation results for those events and a set of software injections that we can use to estimate selection effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb19e09b-0c0f-4755-a053-1865223e1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5ify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3add73-5f25-4560-9a9e-16166847153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "injections = h5ify.load('data/vt.h5')\n",
    "injections = {\n",
    "    k: jnp.array(injections[k], dtype = jnp.float64).squeeze()\n",
    "    for k in injections\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff08a70-c058-4f18-baa4-b51144351b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "injections.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96273869-ae8e-4c2e-b5e4-436a2376e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "posteriors = h5ify.load('data/pe.h5')\n",
    "posteriors = {\n",
    "    k: jnp.array([posteriors[event][k] for event in sorted(posteriors)])\n",
    "    for k in posteriors[list(posteriors)[0]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fd9989-7ace-4287-8cd9-a7b74ca125dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "posteriors.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3a0074-b21d-4189-bf8a-0404ffc8dd05",
   "metadata": {},
   "source": [
    "#### Population model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082b62e-1b48-4aeb-bcc0-079d0775a122",
   "metadata": {},
   "source": [
    "First, let's define the population model that we'll use for the astrophysical distribution of sources.\n",
    "\n",
    "As an example, we'll construct a simple simulated model of hierarchical/repeated/second-generation mergers, where some of the population of black holes are the products of previous mergers.\n",
    "\n",
    "Some of the ideas are following our paper [\"Deep learning and Bayesian inference of gravitational-wave populations: Hierarchical black-hole mergers\" (arXiv:2203.03651)](https://arxiv.org/abs/2203.03651) and code [\"QLUSTER: quick clusters of merging binary black holes\" (arXiv:2305.04987)](https://arxiv.org/abs/2305.04987)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c7d608-1186-447b-826a-d6a133467c27",
   "metadata": {},
   "source": [
    "Below are some utilities we'll use to build the population model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4888f7f-f9eb-48c1-897a-2543afa2a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tapering functions\n",
    "\n",
    "def cubic_filter(x):\n",
    "    return (3 - 2 * x) * x**2 * (0 <= x) * (x <= 1) + (1 < x)\n",
    "\n",
    "def highpass(x, xmin, dmin):\n",
    "    return cubic_filter((x - xmin) / dmin)\n",
    "\n",
    "def lowpass(x, xmax, dmax):\n",
    "    return highpass(x, xmax, -dmax)\n",
    "\n",
    "def bandpass(x, xmin, xmax, dmin, dmax):\n",
    "    return highpass(x, xmin, dmin) * lowpass(x, xmax, dmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ea88c5-da36-4274-aec7-4d2b03ed81f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# power law functions\n",
    "\n",
    "def truncated_powerlaw(x, alpha, xmin, xmax):\n",
    "    cut = (xmin <= x) * (x <= xmax)\n",
    "    shape = x**alpha\n",
    "    norm = (xmax**(alpha + 1) - xmin**(alpha + 1)) / (alpha + 1)\n",
    "    return cut * shape / norm\n",
    "\n",
    "def powerlaw_integral(x, alpha, loc, delta):\n",
    "    a, c, d = alpha, loc, delta\n",
    "    return (\n",
    "        3 * (2 * c + (4 + a) * d)\n",
    "        * (c**2 / (1 + a) - 2 * c * x / (2 + a) + x**2 / (3 + a))\n",
    "        - 2 * (x - c)**3\n",
    "    ) * x**(1 + a) / (4 + a) / d**3\n",
    "\n",
    "def bandpass_powerlaw(x, alpha, xmin, xmax, dmin, dmax):\n",
    "    cut = (xmin <= x) * (x <= xmax)\n",
    "    shape = x**alpha * bandpass(x, xmin, xmax, dmin, dmax)\n",
    "    norm = (\n",
    "        - powerlaw_integral(xmin, alpha, xmin, dmin)\n",
    "        + powerlaw_integral(xmin + dmin, alpha, xmin, dmin)\n",
    "        - (xmin + dmin)**(alpha + 1) / (alpha + 1)\n",
    "        + (xmax - dmax)**(alpha + 1) / (alpha + 1)\n",
    "        - powerlaw_integral(xmax - dmax, alpha, xmax, -dmax)\n",
    "        + powerlaw_integral(xmax, alpha, xmax, -dmax)\n",
    "    )\n",
    "    return cut * shape / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc2bfc-f832-4998-b1cd-a4c6028147f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian functions\n",
    "\n",
    "def truncated_normal(x, mu, sigma, xmin, xmax):\n",
    "    cut = (xmin <= x) * (x <= xmax)\n",
    "    shape = jax.scipy.stats.norm.pdf(x, mu, sigma)\n",
    "    norm = (\n",
    "        - jax.scipy.stats.norm.cdf(xmin, mu, sigma)\n",
    "        + jax.scipy.stats.norm.cdf(xmax, mu, sigma)\n",
    "    )\n",
    "    return cut * shape / norm\n",
    "\n",
    "def normal_integral(x, mu, sigma, loc, delta):\n",
    "    m, s, c, d = mu, sigma, loc, delta\n",
    "    return (\n",
    "        jnp.exp(-(x - m)**2 / 2 / s ** 2) * (2 / jnp.pi)**0.5 * s * (\n",
    "            6 * c * (c + d - m - x)\n",
    "            - 3 * d * (m + x)\n",
    "            + 2 * (m**2 + 2 * s**2 + m * x + x**2)\n",
    "        )\n",
    "        - jax.lax.erf((m - x) / s / 2**0.5) * (\n",
    "            (2 * c + 3 * d - 2 * m) * (c - m)**2\n",
    "            + 3 * s**2 * (2 * c + d - 2 * m)\n",
    "        )\n",
    "    ) / 2 / d**3\n",
    "\n",
    "def bandpass_normal(x, mu, sigma, xmin, xmax, dmin, dmax):\n",
    "    cut = (xmin <= x) * (x <= xmax)\n",
    "    shape = (\n",
    "        jax.scipy.stats.norm.pdf(x, mu, sigma)\n",
    "        * bandpass(x, xmin, xmax, dmin, dmax)\n",
    "    )\n",
    "    norm = (\n",
    "        - normal_integral(xmin, mu, sigma, xmin, dmin)\n",
    "        + normal_integral(xmin + dmin, mu, sigma, xmin, dmin)\n",
    "        - jax.scipy.stats.norm.cdf(xmin + dmin, mu, sigma)\n",
    "        + jax.scipy.stats.norm.cdf(xmax - dmax, mu, sigma)\n",
    "        - normal_integral(xmax - dmax, mu, sigma, xmax, -dmax)\n",
    "        + normal_integral(xmax, mu, sigma, xmax, -dmax)\n",
    "    )\n",
    "    return cut * shape / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f999ec94-e0d3-4441-a354-138efea1c3fc",
   "metadata": {},
   "source": [
    "We'll make the simplifying assumption that the merger rate over comoving volume and source-frame time has a shared redshift evolution for all sources. We'll use [wcosmo](https://github.com/ColmTalbot/wcosmo), which is a nice package for cosmological calculations in JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eaf8d1-fb4f-42c5-892b-5401b1f170ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wcosmo\n",
    "wcosmo.disable_units()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb2425f-ed52-4ad6-8316-084887ab1cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_z(z, parameters):\n",
    "    return (1 + z)**parameters['gamma']\n",
    "\n",
    "def pdf_z(z, parameters):\n",
    "    zmax = 2\n",
    "    fn = lambda z: (\n",
    "        shape_z(z, parameters)\n",
    "        * wcosmo.Planck15.differential_comoving_volume(z) * 4 * jnp.pi / 1e9\n",
    "    )\n",
    "    cut = (0 < z) * (z <= zmax)\n",
    "    shape = fn(z)\n",
    "    zz = jnp.linspace(0, zmax, 10_000)\n",
    "    norm = jnp.trapezoid(fn(zz), zz)\n",
    "    return cut * shape / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83878661-d5fa-4aab-b42a-28994612d456",
   "metadata": {},
   "source": [
    "To build up a population of second-generation mergers, we'll begin by defining the distribution of first-generation mergers. We'll assume that:\n",
    "- black-hole masses are distributed from a tapered power law with an additional Gaussian peak (similar to the [Power Law + Peak](https://arxiv.org/abs/1801.02699) model),\n",
    "- binary mass ratios are follow a tapered power law,\n",
    "- black-holes spins are uniform in mangnitude and isotropic in direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33654819-b3a4-40ef-88a7-d15e6f522cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary mass\n",
    "def pdf_m(m, parameters):\n",
    "    pl = bandpass_powerlaw(\n",
    "        m,\n",
    "        parameters['alpha'],\n",
    "        parameters['m_min'],\n",
    "        parameters['m_max'],\n",
    "        parameters['d_min'],\n",
    "        parameters['d_max'],\n",
    "    )\n",
    "    tn = bandpass_normal(\n",
    "        m,\n",
    "        parameters['mu_m'],\n",
    "        parameters['sigma_m'],\n",
    "        parameters['m_min'],\n",
    "        parameters['m_max'],\n",
    "        parameters['d_min'],\n",
    "        parameters['d_max'],\n",
    "    )\n",
    "    return (1 - parameters['f_m']) * pl + parameters['f_m'] * tn\n",
    "\n",
    "# mass ratio - this is a bit of a handful, but otherwise, autodiff doesn't work\n",
    "# let me know if you spot a better way to do it :')\n",
    "def pdf_q_given_m(q, m, parameters):\n",
    "    # pdf defined in terms if secondary mass, then converted to mass ratio\n",
    "    pdf = lambda q, m: highpass_powerlaw(\n",
    "        q * m, parameters['beta'], parameters['m_min'], m, parameters['d_min'],\n",
    "    ) * m\n",
    "    single = lambda q, m: jax.lax.cond(\n",
    "        parameters['m_min'] <= q * m, lambda: pdf(q, m), lambda: 0.0,\n",
    "    )\n",
    "    return jax.vmap(single)(q.ravel(), m.ravel()).reshape(q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c1e987-5df4-4ac3-a743-0be492dbe071",
   "metadata": {},
   "source": [
    "Next, we need a function to output the properties of black-hole merger remnants from input pre-merger binary properties. Importantly, the remnant recieves a recoil - or \"kick\" - that imparts a velocity, which can eject it from its host environment and thus prevent a second-generation merger.\n",
    "\n",
    "We follow the fitting formulae to numerical-relativity simulations included in the [precession](https://arxiv.org/abs/1605.01067) Python [package](https://github.com/dgerosa/precession), but with a minimal re-implementation in JAX (I've removed the internal vectorization, which can be done automatically with `jax.vmap`, and fixed to the default spin and kick options)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bdaf07-0ba2-46be-93d1-3193cce67c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_eta(q):\n",
    "    return q / (1 + q)**2\n",
    "\n",
    "def eval_theta12(theta1, theta2, deltaphi):\n",
    "    return jnp.arccos(\n",
    "        jnp.sin(theta1) * jnp.sin(theta2) * jnp.cos(deltaphi)\n",
    "        + jnp.cos(theta1) * jnp.cos(theta2)\n",
    "    )\n",
    "\n",
    "def angles_to_Lframe(theta1, theta2, deltaphi, r, q, chi1, chi2):\n",
    "    L = r**0.5 * q / (1 + q)**2\n",
    "    S1 = chi1 / (1 + q)**2\n",
    "    S2 = chi2 * q**2 / (1 + q)**2\n",
    "\n",
    "    Lx = 0\n",
    "    Ly = 0\n",
    "    Lz = L\n",
    "    Lvec = jnp.array([Lx, Ly, Lz])\n",
    "\n",
    "    S1x = S1 * jnp.sin(theta1)\n",
    "    S1y = 0\n",
    "    S1z = S1 * jnp.cos(theta1)\n",
    "    S1vec = jnp.array([S1x, S1y, S1z])\n",
    "\n",
    "    S2x = S2 * jnp.sin(theta2) * jnp.cos(deltaphi)\n",
    "    S2y = S2 * jnp.sin(theta2) * jnp.sin(deltaphi)\n",
    "    S2z = S2 * jnp.cos(theta2)\n",
    "    S2vec = jnp.array([S2x, S2y, S2z])\n",
    "\n",
    "    return Lvec, S1vec, S2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0831d406-b68a-4c16-9dec-2da019e27bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remnantmass(theta1, theta2, q, chi1, chi2):\n",
    "    eta = eval_eta(q)\n",
    "\n",
    "    chit_par =  ( chi2*q**2 * jnp.cos(theta2) + chi1*jnp.cos(theta1) ) / (1+q)**2\n",
    "\n",
    "    #Final mass. Barausse Morozova Rezzolla 2012\n",
    "    p0 = 0.04827\n",
    "    p1 = 0.01707\n",
    "    Z1 = 1 + (1-chit_par**2)**(1/3)* ((1+chit_par)**(1/3)+(1-chit_par)**(1/3))\n",
    "    Z2 = (3* chit_par**2 + Z1**2)**(1/2)\n",
    "    risco = 3 + Z2 - jnp.sign(chit_par) * ((3-Z1)*(3+Z1+2*Z2))**(1/2)\n",
    "    Eisco = (1-2/(3*risco))**(1/2)\n",
    "    #Radiated energy, in units of the initial total mass of the binary\n",
    "    Erad = eta*(1-Eisco) + 4* eta**2 * (4*p0+16*p1*chit_par*(chit_par+1)+Eisco-1)\n",
    "    Mfin = 1- Erad # Final mass\n",
    "\n",
    "    return Mfin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a330ff0-8025-4f0d-b98c-738d0f031cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remnantspin(theta1, theta2, deltaphi, q, chi1, chi2):\n",
    "    eta = eval_eta(q)\n",
    "\n",
    "    kfit = jnp.array( [[jnp.nan, 3.39221, 4.48865, -5.77101, -13.0459] ,\n",
    "                      [35.1278, -72.9336, -86.0036, 93.7371, 200.975],\n",
    "                      [-146.822, 387.184, 447.009, -467.383, -884.339],\n",
    "                      [223.911, -648.502, -697.177, 753.738, 1166.89]])\n",
    "    xifit = 0.474046\n",
    "\n",
    "    # Calculate K00 from Eq 11\n",
    "    kfit = kfit.at[0,0].set(4**2 * ( 0.68646 - jnp.sum( kfit[1:,0] /(4**(3+jnp.arange(kfit.shape[0]-1)))) - (3**0.5)/2))\n",
    "\n",
    "    theta12 = eval_theta12(theta1, theta2, deltaphi)\n",
    "\n",
    "    eps1 = 0.024\n",
    "    eps2 = 0.024\n",
    "    eps12 = 0\n",
    "    theta1 = theta1 + eps1 * jnp.sin(theta1)\n",
    "    theta2 = theta2 + eps2 * jnp.sin(theta2)\n",
    "    theta12 = theta12 + eps12 * jnp.sin(theta12)\n",
    "\n",
    "    # Eq. 14 - 15\n",
    "    atot = ( chi1*jnp.cos(theta1) + chi2*jnp.cos(theta2)*q**2 ) / (1+q)**2\n",
    "    aeff = atot + xifit*eta* ( chi1*jnp.cos(theta1) + chi2*jnp.cos(theta2) )\n",
    "\n",
    "    # Eq. 2 - 6 evaluated at aeff, as specified in Eq. 11\n",
    "    Z1= 1 + (1-(aeff**2))**(1/3) * ( (1+aeff)**(1/3) + (1-aeff)**(1/3) )\n",
    "    Z2= ( (3*aeff**2) + (Z1**2) )**(1/2)\n",
    "    risco= 3 + Z2 - jnp.sign(aeff) * ( (3-Z1)*(3+Z1+2*Z2) )**(1/2)\n",
    "    Eisco=(1-2/(3*risco))**(1/2)\n",
    "    Lisco = (2/(3*(3**(1/2)))) * ( 1 + 2*(3*risco - 2 )**(1/2) )\n",
    "\n",
    "    # Eq. 13\n",
    "    etatoi = eta**(1+jnp.arange(kfit.shape[0]))\n",
    "    innersum = jnp.sum(kfit.T * etatoi,axis=1)\n",
    "    aefftoj = aeff**(jnp.arange(kfit.shape[1]))\n",
    "    sumell = jnp.sum(innersum  * aefftoj,axis=0)\n",
    "    ell = jnp.abs( Lisco  - 2*atot*(Eisco-1)  + sumell )\n",
    "\n",
    "    # Eq. 16\n",
    "    chifin = (1/(1+q)**2) * ( chi1**2 + (chi2**2)*(q**4)  + 2*chi1*chi2*(q**2)*jnp.cos(theta12)\n",
    "            + 2*(chi1*jnp.cos(theta1) + chi2*(q**2)*jnp.cos(theta2))*ell*q + ((ell*q)**2)  )**(1/2)\n",
    "\n",
    "    return jnp.minimum(chifin,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de942ae6-0052-40f5-804d-b873f9b5b2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remnantkick(bigTheta, theta1, theta2, deltaphi, q, chi1, chi2):\n",
    "# kms=False, maxphase=False, superkick=True, hangupkick=True, crosskick=True, full_output=False):\n",
    "\n",
    "    eta = eval_eta(q)\n",
    "\n",
    "    Lvec, S1vec, S2vec = angles_to_Lframe(theta1, theta2, deltaphi, 1, q, chi1, chi2)\n",
    "    hatL = Lvec / jnp.linalg.norm(Lvec)\n",
    "    hatS1 = S1vec / jnp.linalg.norm(S1vec)\n",
    "    hatS2 = S2vec / jnp.linalg.norm(S2vec)\n",
    "\n",
    "    #More spin parameters.\n",
    "    Delta = - 1/(1+q) * (q*chi2*hatS2 - chi1*hatS1)\n",
    "    Delta_par = jnp.dot(Delta, hatL)\n",
    "    Delta_perp = jnp.linalg.norm(jnp.cross(Delta, hatL))\n",
    "    chit = 1/(1+q)**2 * (chi2*q**2*hatS2 + chi1*hatS1)\n",
    "    chit_par = jnp.dot(chit, hatL)\n",
    "    chit_perp = jnp.linalg.norm(jnp.cross(chit, hatL))\n",
    "\n",
    "    #Coefficients are quoted in km/s\n",
    "    #vm and vperp from Kesden at 2010a. vpar from Lousto Zlochower 2013\n",
    "    zeta=jnp.radians(145)\n",
    "    A=1.2e4\n",
    "    B=-0.93\n",
    "    H=6.9e3\n",
    "\n",
    "    #Multiply by 0/1 boolean flags to select terms\n",
    "    V11 = 3677.76\n",
    "    VA = 2481.21\n",
    "    VB = 1792.45\n",
    "    VC = 1506.52\n",
    "    C2 = 1140\n",
    "    C3 = 2481\n",
    "\n",
    "    # #maxkick\n",
    "    # bigTheta=np.random.uniform(0, 2*np.pi,q.shape) * (not maxphase)\n",
    "\n",
    "    vm = A * eta**2 * (1+B*eta) * (1-q)/(1+q)\n",
    "    vperp = H * eta**2 * Delta_par\n",
    "    vpar = 16*eta**2 * (Delta_perp * (V11 + 2*VA*chit_par + 4*VB*chit_par**2 + 8*VC*chit_par**3) + chit_perp * Delta_par * (2*C2 + 4*C3*chit_par)) * jnp.cos(bigTheta)\n",
    "    kick = jnp.array([vm+vperp*jnp.cos(zeta),vperp*jnp.sin(zeta),vpar]).T\n",
    "\n",
    "    # if not kms:\n",
    "    #     kick = kick/299792.458 # speed of light in km/s\n",
    "\n",
    "    vk = jnp.linalg.norm(kick)\n",
    "\n",
    "    return vk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e541a90-3c9f-4037-97d3-09c9e42f109d",
   "metadata": {},
   "source": [
    "Let's check this matches the original code. Note that [precession](https://github.com/dgerosa/precession) resamples the additional angle $\\Theta$ internally, but above we made it an explicit input. We seed the random sampling below to ensure the same values are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda9d65-b7cc-43a9-894e-532c2c873a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import precession\n",
    "from corner import corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1814d6ed-9bc4-46e2-873b-9cd8f6e2f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10_000\n",
    "np.random.seed(0)\n",
    "bigTheta = np.random.uniform(0, 2 * np.pi, n)\n",
    "theta1 = np.random.uniform(0, np.pi, n)\n",
    "theta2 = np.random.uniform(0, np.pi, n)\n",
    "deltaphi = np.random.uniform(0, 2 * np.pi, n)\n",
    "q = np.random.uniform(0.1, 1, n)\n",
    "chi1 = np.random.uniform(0, 1, n)\n",
    "chi2 = np.random.uniform(0, 1, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeae059d-7842-4e1c-bd85-50ba47db9373",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "og = np.array([\n",
    "    precession.remnantmass(theta1, theta1, q, chi1, chi2),\n",
    "    precession.remnantspin(theta1, theta1, deltaphi, q, chi1, chi2),\n",
    "    precession.remnantkick(theta1, theta1, deltaphi, q, chi1, chi2, kms = True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d43936-26f0-4f26-ab27-c6259d24e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "re = np.array([\n",
    "    jax.vmap(remnantmass)(theta1, theta1, q, chi1, chi2),\n",
    "    jax.vmap(remnantspin)(theta1, theta1, deltaphi, q, chi1, chi2),\n",
    "    jax.vmap(remnantkick)(bigTheta, theta1, theta1, deltaphi, q, chi1, chi2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07950808-6500-47c8-b5c5-8296d29b4144",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(og, re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7483abf2-f7af-4f78-81b9-6847374dbf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = None\n",
    "for i, (samples, ls) in enumerate(zip((og, re), ('-', '--'))):\n",
    "    fig = corner(\n",
    "        samples.T, labels = ('mf', 'af', 'vf'), fig = fig,\n",
    "        plot_datapoints = False, plot_density = False,\n",
    "        plot_contours = True, fill_contours = False, no_fill_contours = True,\n",
    "        hist_kwargs = dict(density = True, color = f'C{i}', ls = ls),\n",
    "        contour_kwargs = dict(colors = [f'C{i}'], linestyles = ls),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abaa501-1202-476d-8a3e-a8f4d14bf7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8d8725c-8c42-4c61-b351-59c6e491e9bf",
   "metadata": {},
   "source": [
    "#### Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8460ae8-44a7-429f-9564-11d517a93b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9865803-5c07-41c4-99b5-6f2b352febd5",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe329f0-b101-4543-8a54-c651ecf7e34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2196f811-d29a-444b-8d86-6638e4bf7d61",
   "metadata": {},
   "source": [
    "#### Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25348b04-ad08-4c92-b74a-7dae89f2a68d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47e7a4a0-52e0-4d4f-886e-b20362d2eb7e",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcfb2ae-dae6-4304-96fd-718fc32773d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f6bcbfd-56ca-4e3f-baff-97c056df7870",
   "metadata": {},
   "source": [
    "#### Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7253dcf9-9972-42c6-995c-536ec4d158ed",
   "metadata": {},
   "source": [
    "We should add a bit more realism to our model:\n",
    "- Include more flexible spin distributions.\n",
    "- Account for differing time delays and redshift evolution for first- and second-generaiton mergers.\n",
    "- Allow for a subpopulation of binaries in which both black holes are second generation.\n",
    "- Allow for a subpopulation of sources in which second-generation mergers can't happen at all with distinct parameters for it's mass distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d698d5c0-0f44-470c-b9c2-8210347606a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0e6db3-5c8d-44b4-bac7-29e109ebccb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81932ee-f0c3-4be5-b659-6f1a1a84aedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753b77c2-c34d-4bae-a3e6-d035daac5c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba8bfb2-686f-44e6-9a6d-dee731930495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary mass\n",
    "def pdf_m(m, parameters):\n",
    "    pl = bandpass_powerlaw(\n",
    "        m,\n",
    "        parameters['alpha'],\n",
    "        parameters['m_min'],\n",
    "        parameters['m_max'],\n",
    "        parameters['d_min'],\n",
    "        parameters['d_max'],\n",
    "    )\n",
    "    tn = bandpass_normal(\n",
    "        m,\n",
    "        parameters['mu_m'],\n",
    "        parameters['sigma_m'],\n",
    "        parameters['m_min'],\n",
    "        parameters['m_max'],\n",
    "        parameters['d_min'],\n",
    "        parameters['d_max'],\n",
    "    )\n",
    "    return (1 - parameters['f_m']) * pl + parameters['f_m'] * tn\n",
    "\n",
    "# mass ratio - this is a bit of a handful, but otherwise, autodiff doesn't work\n",
    "# let me know if you spot a better way to do it :')\n",
    "def pdf_q_given_m(q, m, parameters):\n",
    "    # pdf defined in terms if secondary mass, then converted to mass ratio\n",
    "    pdf = lambda q, m: highpass_powerlaw(\n",
    "        q * m, parameters['beta'], parameters['m_min'], m, parameters['d_min'],\n",
    "    ) * m\n",
    "    single = lambda q, m: jax.lax.cond(\n",
    "        parameters['m_min'] <= q * m, lambda: pdf(q, m), lambda: 0.0,\n",
    "    )\n",
    "    return jax.vmap(single)(q.ravel(), m.ravel()).reshape(q.shape)\n",
    "\n",
    "# spin magnitude\n",
    "def pdf_a(a, parameters):\n",
    "    return truncnorm(a, parameters['mu_a'], parameters['sigma_a'], 0, 1)\n",
    "\n",
    "# spin tilt\n",
    "def pdf_c(c, parameters):\n",
    "    return truncnorm(c, parameters['mu_c'], parameters['sigma_c'], -1, 1)\n",
    "\n",
    "# redshift\n",
    "def shape_z(z, parameters):\n",
    "    return (1 + z)**parameters['gamma']\n",
    "\n",
    "def pdf_z(z, parameters):\n",
    "    zmax = 2\n",
    "    fn = lambda z: (\n",
    "        shape_z(z, parameters)\n",
    "        * wcosmo.Planck15.differential_comoving_volume(z) * 4 * jnp.pi / 1e9\n",
    "    )\n",
    "    cut = (0 < z) * (z <= zmax)\n",
    "    shape = fn(z)\n",
    "    zz = jnp.linspace(0, zmax, 10_000)\n",
    "    norm = jnp.trapezoid(fn(zz), zz)\n",
    "    return cut * shape / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb1e9f-3381-4573-8fdf-9a4c37bc29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the combined probability density for the population model\n",
    "def density(data, parameters):\n",
    "    return (\n",
    "        pdf_m(data['mass_1_source'], parameters)\n",
    "        * pdf_q_given_m(data['mass_ratio'], data['mass_1_source'], parameters)\n",
    "        # * pdf_q(data['mass_ratio'], parameters)\n",
    "        * pdf_a(data['a_1'], parameters)\n",
    "        * pdf_a(data['a_2'], parameters)\n",
    "        * pdf_c(data['cos_tilt_1'], parameters)\n",
    "        * pdf_c(data['cos_tilt_2'], parameters)\n",
    "        * pdf_z(data['redshift'], parameters)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88a8379-ee9c-400e-8656-7950bd2a49be",
   "metadata": {},
   "source": [
    "Let's plot what the population models look like for some parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944182d1-2a62-4022-9900-28a453a71bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3efc544-b874-4b8e-9bbb-785be878db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict(\n",
    "    alpha = -3.5,\n",
    "    m_min = 5,\n",
    "    m_max = 80,\n",
    "    d_min = 5,\n",
    "    d_max = 10,\n",
    "    mu_m = 35,\n",
    "    sigma_m = 3,\n",
    "    f_m = 0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e926564-ca63-4c6b-8db5-51364144746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = jnp.linspace(2, 100, 1_000)\n",
    "p = pdf_m(m, parameters)\n",
    "\n",
    "plt.plot(m, p)\n",
    "plt.semilogy()\n",
    "plt.xlabel('primary mass')\n",
    "plt.ylabel('PDF')\n",
    "plt.ylim(1e-5, 1e0)\n",
    "\n",
    "print(jnp.trapezoid(p, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6682f45c-93f3-4758-9aad-ee2f391fd1e4",
   "metadata": {},
   "source": [
    "#### Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e870f9-0fcf-4c36-94f8-304978e74b93",
   "metadata": {},
   "source": [
    "Next, we'll set priors on the parameters of the population model - these are the parameters we want to measure from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3ab3e8-61f5-438a-b362-5a99de5703b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe85080-17da-4afe-8c33-a79c33437a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = dict(\n",
    "    alpha = numpyro.distributions.Uniform(-10, 10),\n",
    "    m_min = numpyro.distributions.Uniform(2, 6),\n",
    "    m_max = numpyro.distributions.Uniform(70, 100),\n",
    "    d_min = numpyro.distributions.Uniform(0, 10),\n",
    "    d_max = numpyro.distributions.Uniform(0, 10),\n",
    "    mu_m = numpyro.distributions.Uniform(20, 50),\n",
    "    sigma_m = numpyro.distributions.Uniform(1, 10),\n",
    "    f_m = numpyro.distributions.Uniform(0, 1),\n",
    "    beta = numpyro.distributions.Uniform(-10, 10),\n",
    "    mu_a = numpyro.distributions.Uniform(0, 1),\n",
    "    sigma_a = numpyro.distributions.Uniform(0.1, 1),\n",
    "    mu_c = numpyro.distributions.Uniform(-1, 1),\n",
    "    sigma_c = numpyro.distributions.Uniform(0.1, 4),\n",
    "    gamma = numpyro.distributions.Uniform(-10, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f736a16-e067-4466-8329-b5d7f6dd1d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(priors)\n",
    "dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7406fd5-831f-4b16-a3f0-d3d1475dc867",
   "metadata": {},
   "source": [
    "#### Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1651c356-e767-481e-92f5-27c5a0ce7133",
   "metadata": {},
   "source": [
    "How likely is it that our population model is responsible for the observed data?\n",
    "\n",
    "Unlike neural posterior estimation, which is a simulation-based inference method, variational inference is a likelihood-based method. This also means that it is not amortized, i.e., it is fit to one specific data set. Below we code up the gravitational-wave population likelihood; see, e.g.,\n",
    "\n",
    "- https://arxiv.org/abs/1809.02063,\n",
    "- https://arxiv.org/abs/2007.05579,\n",
    "- https://arxiv.org/abs/2410.19145.\n",
    "\n",
    "In particular, the likelihood function is approximated with several Monte Carlo integrals, which introduces additional statistical variance (https://arxiv.org/abs/1904.10879, https://arxiv.org/abs/2204.00461, https://arxiv.org/abs/2304.06138). We make sure to keep track of this variance below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787f1a08-3461-4c6e-8900-6cc07b2e89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and variance of the mean\n",
    "def mean_and_variance(weights, n):\n",
    "    mean = jnp.sum(weights, axis = -1) / n\n",
    "    variance = jnp.sum(weights**2, axis = -1) / n**2 - mean**2 / n\n",
    "    return mean, variance\n",
    "\n",
    "# lazy ln(mean) and variance of ln(mean)\n",
    "def ln_mean_and_variance(weights, n):\n",
    "    mean, variance = mean_and_variance(weights, n)\n",
    "    return jnp.log(mean), variance / mean**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46948f15-411d-4f68-8a98-3265be4c0407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_likelihood_and_variance(posteriors, injections, density, parameters):\n",
    "    pe_weights = density(posteriors, parameters) / posteriors['prior']\n",
    "    vt_weights = density(injections, parameters) / injections['prior']\n",
    "    num_obs, num_pe = pe_weights.shape\n",
    "    ln_lkls, pe_variances = ln_mean_and_variance(pe_weights, num_pe)\n",
    "    ln_pdet, vt_variance = ln_mean_and_variance(vt_weights, injections['total'])\n",
    "    ln_lkl = ln_lkls.sum() - ln_pdet * num_obs\n",
    "    variance = pe_variances.sum() + vt_variance * num_obs**2\n",
    "    # ln_lkl = jnp.nan_to_num(ln_lkl, nan = -jnp.inf)\n",
    "    # variance = jnp.nan_to_num(variance, nan = jnp.inf)\n",
    "    return ln_lkl, variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71b3d01-eaed-4b84-91a3-baff0c703bc1",
   "metadata": {},
   "source": [
    "#### Normalizing flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6904f4a1-2d11-4128-b338-bd740edd7117",
   "metadata": {},
   "source": [
    "Now let's set up the model that we'll train. We'll use a [block neural autoregressive flow](https://arxiv.org/abs/1904.04676) to approximate the population posterior. There's a nice library called [flowjax](https://github.com/danielward27/flowjax) to do normalizing flows in JAX that we'll use. This is built on top of [equinox](https://github.com/patrick-kidger/equinox), which will handle our neural networks.\n",
    "\n",
    "If you aren't familiar with normalizing flows, the (very brief) idea is that you can construct a probability distribution by transforming a simple known distribution (such as a standard normal distribution) with an invertible and differentiable function using the change-of-variables formula. For normalizing flows, that function is parametrized by a neural network, which is what makes the transformation flexible.\n",
    "\n",
    "The transformation is trained so that the output distribution best matches some target distribution - in our case the population posterior distribution. The variables being transformed are the population parameters above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7c1835-f135-4326-b0da-c0585e13e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox\n",
    "from flowjax.distributions import StandardNormal\n",
    "from flowjax.flows import block_neural_autoregressive_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15863a35-02f8-4b81-87d1-2e6d29891d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_init = block_neural_autoregressive_flow(\n",
    "    key = jax.random.key(0),\n",
    "    base_dist = StandardNormal(shape = (dim,)),\n",
    "    invert = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c011ea-9599-47ca-aab8-b27186d8fed3",
   "metadata": {},
   "source": [
    "We should take care that our normalizing flow is defined on the parameter domain we want it to be. In particular, our priors impose bounds on the range of values that can be taken. Therefore, we'll add some additional transformations to ensure those bounds are respected. These transformation are fixed and not trainable, unlike the flow transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0b4cab-2f98-4787-859b-82ade017eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flowjax.bijections import Affine, Sigmoid, Chain, Stack\n",
    "from flowjax.distributions import Transformed\n",
    "import paramax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1f938f-5afa-45ab-827c-f305ab3b1ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bijections = []\n",
    "for k in priors:\n",
    "    lo, hi = priors[k].low, priors[k].high\n",
    "    bijection = Chain([Sigmoid(), Affine(loc = lo, scale = hi - lo)])\n",
    "    bijections.append(bijection)\n",
    "flow_init = Transformed(flow_init, paramax.non_trainable(Stack(bijections)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e01d02-1955-4c6d-987f-6e518a82aee2",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3e2d6a-caf3-44b5-a632-f14e9885cbb4",
   "metadata": {},
   "source": [
    "To train the flow, we need to define a loss function to minimize with respect to the neural-network parameters. For variational inference, the most common choice is the Kullback-Leibler divergence from the target posterior $\\mathcal{P}(\\Lambda)$ to the normalizing flow approximation $\\mathcal{Q}(\\Lambda)$, with $\\Lambda$ being the parameters of our population model that we want to infer:\n",
    "\n",
    "$$\n",
    "\\mathrm{KL}[\\mathcal{Q},\\mathcal{P}] = \\int \\mathrm{d}\\,\\Lambda\\, \\mathcal{Q}(\\Lambda) \\ln \\frac{\\mathcal{Q}(\\Lambda)}{\\mathcal{P}(\\Lambda)} .\n",
    "$$\n",
    "\n",
    "We know that our target posterior can be written using Bayes' theorem:\n",
    "\n",
    "$$\n",
    "\\mathcal{P}(\\Lambda) = \\frac { \\mathcal{L}(\\Lambda) \\pi(\\Lambda) } { \\mathcal{Z} } ,\n",
    "$$\n",
    "\n",
    "where $\\mathcal{L}(\\Lambda)$ is the likelihood function, $\\pi(\\Lambda)$ is the prior, and $\\mathcal{Z} = \\int \\mathrm{d}\\,\\Lambda\\, \\mathcal{L}(\\Lambda) \\pi(\\Lambda)$ is the evidence.\n",
    "\n",
    "We know the likelihood and prior - they're above - but we don't know the evidence. Therefore, the equivalent loss function is used:\n",
    "\n",
    "$$\n",
    "L = \\mathrm{KL}[\\mathcal{Q},\\mathcal{P}] - \\ln\\mathcal{Z} = \\int \\mathrm{d}\\,\\Lambda\\, \\mathcal{Q}(\\Lambda) \\ln \\frac{ \\mathcal{Q}(\\Lambda) }{ \\mathcal{L}(\\Lambda) \\pi(\\Lambda) } .\n",
    "$$\n",
    "\n",
    "This can be approximate with Monte Carlo integration using a batch of $M$ samples $\\{\\Lambda_i\\}_{i=1}^M$ drawn from the normalizing flow $\\mathcal{Q}$:\n",
    "\n",
    "$$\n",
    "L \\approx \\frac{1}{M} \\sum_{i=1}^M \\ln \\frac{ \\mathcal{Q}(\\Lambda_i) }{ \\mathcal{L}(\\Lambda_i) \\pi(\\Lambda_i) } .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6a876-4993-4072-9de2-b16b3d2d2952",
   "metadata": {},
   "source": [
    "First, let's choose some training settings. We'll use [optax](https://github.com/google-deepmind/optax) to optimize the neural-network parameters and [jax_tqdm](https://github.com/jeremiecoullon/jax-tqdm) to add a progress bar to our loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4d7146-9f96-4402-8d9e-f0b76f0c776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "import jax_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b658605c-30c5-4f49-ac53-52564adc35d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1 # perhaps surprisingly, this is sufficient\n",
    "steps = 10_000\n",
    "learning_rate = 1e-2\n",
    "optimizer = optax.adam(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a7c37-03d9-4968-af32-f56f4ad0a652",
   "metadata": {},
   "source": [
    "Now the loss function and training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb407410-d791-4609-91f1-402f5fe4acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the flow intro trainable and non-trainable partitions\n",
    "params_init, static = equinox.partition(\n",
    "    pytree = flow_init,\n",
    "    filter_spec = equinox.is_inexact_array,\n",
    "    is_leaf = lambda leaf: isinstance(leaf, paramax.NonTrainable),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919c7b41-d4a0-4986-894e-181deca2d82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params, key):\n",
    "    flow = equinox.combine(params, static)\n",
    "    samples, ln_flows = flow.sample_and_log_prob(key, (batch_size,))\n",
    "    parameters = dict(zip(priors, samples.T))\n",
    "    ln_lkls, variances = jax.vmap(\n",
    "        lambda parameters: ln_likelihood_and_variance(\n",
    "            posteriors, injections, density, parameters,\n",
    "        ),\n",
    "    )(parameters)\n",
    "    ln_priors = jnp.sum(\n",
    "        jnp.array([priors[k].log_prob(parameters[k]) for k in priors]),\n",
    "        axis = 0,\n",
    "    )\n",
    "    return jnp.mean(ln_flows - ln_priors - ln_lkls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b1a90f-5c91-4320-b2f7-24653794cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax_tqdm.scan_tqdm(steps, print_rate = 100, tqdm_type = 'std')\n",
    "def update(carry, step):\n",
    "    key, params, state = carry\n",
    "    key, _key = jax.random.split(key)\n",
    "    loss, grad = equinox.filter_value_and_grad(loss_fn)(params, _key)\n",
    "    updates, state = optimizer.update(grad, state, params)\n",
    "    params = equinox.apply_updates(params, updates)\n",
    "    carry = key, params, state\n",
    "    return carry, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf2dd5-f0bc-4fb0-b281-7b65bc7f44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22f456-e270-4b3e-80a9-f49d661ad667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, the training loop.\n",
    "# Sometimes the initial JIT compilation takes a few second to get going...\n",
    "\n",
    "state = optimizer.init(params_init)\n",
    "carry = jax.random.key(1), params_init, state\n",
    "\n",
    "t0 = time.time()\n",
    "carry, losses = jax.lax.scan(update, carry, jnp.arange(steps))\n",
    "dt = time.time() - t0\n",
    "print('total time including JIT compilation:', dt)\n",
    "\n",
    "key, params, state = carry\n",
    "flow = equinox.combine(params, static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab00f9-f804-4c22-b1ad-d6cd3365b634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss function values over training steps\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d63efe9-e18f-4535-9297-a6e69a46795e",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed36c1b1-05ff-4093-988e-b9c1e3b493b3",
   "metadata": {},
   "source": [
    "Now that the flow is trained, we can draw as many posterior samples as we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de8063-2fd4-4b48-bb56-9531aafd1318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corner import corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec47c9-aa00-4ea6-b884-655264303eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = flow.sample(jax.random.key(2), (10_000,))\n",
    "posterior = dict(zip(priors, samples.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f97ce4-406f-4313-b781-3a57f1e9ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "corner(np.array(samples), labels = list(priors));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a420b772-e2fa-441a-a76b-a0a516a6bbf7",
   "metadata": {},
   "source": [
    "Let's also plot the inferred population-level distributions of source parameters and their posterior uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e0f1d0-e332-4301-a0d8-777aad91f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = dict(\n",
    "    mass_1_source = jnp.linspace(2, 100, 1_000),\n",
    "    mass_ratio = jnp.linspace(0, 1, 1_000),\n",
    "    a = jnp.linspace(0, 1, 1_000),\n",
    "    cos_tilt = jnp.linspace(-1, 1, 1_000),\n",
    "    redshift = jnp.linspace(0, 2, 1_000),\n",
    ")\n",
    "\n",
    "# the mass ratio model is conditional on the primary mass, so we have to marginalize\n",
    "def pdf_q_marginal(q, parameters):\n",
    "    x, y = jnp.meshgrid(q, grid['mass_1_source'], indexing = 'ij')\n",
    "    p = pdf_q_given_m(x, y, parameters) * pdf_m(y, parameters)\n",
    "    return jnp.trapezoid(p, y, axis = 1)\n",
    "\n",
    "pdf = dict(\n",
    "    mass_1_source = pdf_m,\n",
    "    mass_ratio = pdf_q_marginal,\n",
    "    a = pdf_a,\n",
    "    cos_tilt = pdf_c,\n",
    "    redshift = pdf_z,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7591a78a-94a0-4286-9d7d-e36b77d8684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(k, data):\n",
    "    # we use sequential map for mass ratio because the integral uses more memory\n",
    "    single = lambda parameters: pdf[k](grid[k], parameters)\n",
    "    if k == 'mass_ratio':\n",
    "        ps = jax.lax.map(single, data)\n",
    "    else:\n",
    "        ps = jax.vmap(single)(data)\n",
    "\n",
    "    for qs, alpha in (\n",
    "        ((0.005, 0.995), 0.2),\n",
    "        ((0.05, 0.95), 0.3),\n",
    "        ((0.25, 0.75), 0.4),\n",
    "    ):\n",
    "        label = f'{(qs[1]-qs[0]) * 100:.0f}% posterior'\n",
    "        plt.fill_between(\n",
    "            grid[k], *np.quantile(ps, qs, axis = 0), label = label,\n",
    "            color = 'C0', alpha = alpha, lw = 0,\n",
    "        )\n",
    "\n",
    "    plt.plot(\n",
    "        grid[k], np.median(ps, axis = 0), label = 'median posterior',\n",
    "        c = 'C1', lw = 2,\n",
    "    )\n",
    "    plt.plot(\n",
    "        grid[k], np.mean(ps, axis = 0), label = 'mean posterior (PPD)',\n",
    "        c = 'C2', lw = 2, ls = '--',\n",
    "    )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(k)\n",
    "    plt.ylabel(f'p({k})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42cda2a-47a9-453b-bdc2-894540ec4e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in 'mass_1_source', 'mass_ratio', 'a', 'cos_tilt', 'redshift':\n",
    "    make_plot(k, posterior)\n",
    "\n",
    "    if k == 'mass_1_source':\n",
    "        plt.semilogy()\n",
    "        plt.ylim(1e-5, 1e0)\n",
    "    elif k == 'mass_ratio':\n",
    "        plt.semilogy()\n",
    "        plt.ylim(1e-2, 1e1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb18011-d3fa-4e2a-88d7-21bdd4da5f00",
   "metadata": {},
   "source": [
    "#### Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16da4c9b-8aa9-4bde-8840-57c4656eeac4",
   "metadata": {},
   "source": [
    "There is some immediate tinkering you can do with the code above:\n",
    "- The training settings, e.g., batch size, number of training steps, learning rate, optimizer etc. Try a [learning-rate scheduler](https://optax.readthedocs.io/en/latest/api/optimizer_schedules.html), for example.\n",
    "- The [flow settings](https://danielward27.github.io/flowjax/api/flows.html#flowjax.flows.block_neural_autoregressive_flow), e.g., try making the network smaller or larger.\n",
    "- The flow itself, i.e., try [a different type](https://danielward27.github.io/flowjax/api/flows.html) of normalizing flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71caa29d-d355-4f7f-8183-c65ff3086ddd",
   "metadata": {},
   "source": [
    "Try targeting a different gravitational-wave population:\n",
    "- Try altering some of the population models.\n",
    "- Try a completely different population models that you're interested in testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09dedda-1aed-4a13-98b3-f25f9715c270",
   "metadata": {},
   "source": [
    "You could compare to different code backends or packages:\n",
    "- Try implementing this in your favourite ML package, e.g., PyTorch, TensorFlow, Julia, etc.\n",
    "- Compare the results here to an existing variational inference package, e.g., [pyro](https://docs.pyro.ai/en/stable/inference_algos.html) or [numpyro](https://num.pyro.ai/en/latest/svi.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a681af54-e328-47fc-8dc4-97a49b609776",
   "metadata": {},
   "source": [
    "We should check our results:\n",
    "- Do you trust the posterior predicted by the flow and how would you test it?\n",
    "- How could you use the normalizing flow to compute the Bayesian evidence for model comparison?\n",
    "- What about the Monte Carlo variance - is it under control?\n",
    "- Try reusing the likelihood function we coded up with a stochastic sampling algorithm to compare posteriors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be1ead-c594-448c-b117-c9908aadc5ac",
   "metadata": {},
   "source": [
    "In https://arxiv.org/abs/2504.07197, we have several tips for training and inference validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2087bd91-06ca-4040-be3d-fc5e4a319f18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
