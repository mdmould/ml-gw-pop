{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05629f79-710d-42f1-a2ff-ba19ae032ab7",
   "metadata": {},
   "source": [
    "If you're running in a separate notebook (e.g., Google Colab), go through and un-comment the cells below as required. Also make sure to set the runtime before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef958a-7187-4607-93a5-e025b1d5b5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're running on a shared cluster and want to limit the resources you take up:\n",
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = '1'\n",
    "os.environ[\"MKL_NUM_THREADS\"] = '1'\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = '1'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = '1'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['NPROC'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # you can change to a GPU ID not in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee84ca38-867c-4a27-b469-7bb8f11ce30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy matplotlib corner h5ify precession\n",
    "# !pip install wcosmo jax_tqdm equinox equinox optax flowjax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755fdc9a-fa7d-4ae3-83da-2e7f49b8c76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you're running on CPU:\n",
    "# !pip install jax numypro\n",
    "\n",
    "# # If you're running on GPU\n",
    "# !pip install -U 'jax[cuda12]'\n",
    "# !pip install 'numpyro[cuda]' -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b7abc-c905-476e-9510-0eac7d81ad69",
   "metadata": {},
   "source": [
    "## Simulation-based population model of gravitational-wave sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40221c43-7a01-4d01-a75a-739acfb5966c",
   "metadata": {},
   "source": [
    "In this notebook, we'll train a normalizing flow to learn an astrophysical population model using simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff7e353-bfa8-46e2-bd82-a2fe5e82bb5f",
   "metadata": {},
   "source": [
    "We'll use [JAX](https://github.com/jax-ml/jax) as the main workhorse behind this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba92f396-e8d9-4eff-aecd-d954c2e7e9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "jax.config.update('jax_enable_x64', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8872384d-c062-4889-9bdc-30841d4dcb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for GPU devices\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3a0074-b21d-4189-bf8a-0404ffc8dd05",
   "metadata": {},
   "source": [
    "#### Population model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082b62e-1b48-4aeb-bcc0-079d0775a122",
   "metadata": {},
   "source": [
    "First, let's define the population model that we'll use for the astrophysical distribution of sources.\n",
    "\n",
    "As an example, we'll construct a simple simulated model of \"hierarchical\" mergers, where some of the population of black holes are the products of previous black-hole mergers. We call \"first-generation\" (1G) black holes those that are born from stars and \"second-generation\" (2G) black holes those that are born from black-hole mergers.\n",
    "\n",
    "Some of the ideas are based on our paper [\"Deep learning and Bayesian inference of gravitational-wave populations: Hierarchical black-hole mergers\" (arXiv:2203.03651)](https://arxiv.org/abs/2203.03651) and [code](http://github.com/mdmould/qluster) called [\"QLUSTER: quick clusters of merging binary black holes\" (arXiv:2305.04987)](https://arxiv.org/abs/2305.04987)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c7d608-1186-447b-826a-d6a133467c27",
   "metadata": {},
   "source": [
    "Below are some utilities we'll use to build the population model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4888f7f-f9eb-48c1-897a-2543afa2a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tapering functions\n",
    "\n",
    "def cubic_filter(x):\n",
    "    return (3 - 2 * x) * x**2 * (0 <= x) * (x <= 1) + (1 < x)\n",
    "\n",
    "def highpass(x, xmin, dmin):\n",
    "    return cubic_filter((x - xmin) / dmin)\n",
    "\n",
    "def lowpass(x, xmax, dmax):\n",
    "    return highpass(x, xmax, -dmax)\n",
    "\n",
    "def bandpass(x, xmin, xmax, dmin, dmax):\n",
    "    return highpass(x, xmin, dmin) * lowpass(x, xmax, dmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ea88c5-da36-4274-aec7-4d2b03ed81f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# power law functions\n",
    "\n",
    "def powerlaw_integral(x, alpha, loc, delta):\n",
    "    a, c, d = alpha, loc, delta\n",
    "    return (\n",
    "        3 * (2 * c + (4 + a) * d)\n",
    "        * (c**2 / (1 + a) - 2 * c * x / (2 + a) + x**2 / (3 + a))\n",
    "        - 2 * (x - c)**3\n",
    "    ) * x**(1 + a) / (4 + a) / d**3\n",
    "\n",
    "def bandpass_powerlaw(x, alpha, xmin, xmax, dmin, dmax):\n",
    "    cut = (xmin <= x) * (x <= xmax)\n",
    "    shape = x**alpha * bandpass(x, xmin, xmax, dmin, dmax)\n",
    "    norm = (\n",
    "        - powerlaw_integral(xmin, alpha, xmin, dmin)\n",
    "        + powerlaw_integral(xmin + dmin, alpha, xmin, dmin)\n",
    "        - (xmin + dmin)**(alpha + 1) / (alpha + 1)\n",
    "        + (xmax - dmax)**(alpha + 1) / (alpha + 1)\n",
    "        - powerlaw_integral(xmax - dmax, alpha, xmax, -dmax)\n",
    "        + powerlaw_integral(xmax, alpha, xmax, -dmax)\n",
    "    )\n",
    "    return cut * shape / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc2bfc-f832-4998-b1cd-a4c6028147f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian functions\n",
    "\n",
    "def truncated_normal(x, mu, sigma, xmin, xmax):\n",
    "    cut = (xmin <= x) * (x <= xmax)\n",
    "    shape = jax.scipy.stats.norm.pdf(x, mu, sigma)\n",
    "    norm = (\n",
    "        - jax.scipy.stats.norm.cdf(xmin, mu, sigma)\n",
    "        + jax.scipy.stats.norm.cdf(xmax, mu, sigma)\n",
    "    )\n",
    "    return cut * shape / norm\n",
    "\n",
    "def normal_integral(x, mu, sigma, loc, delta):\n",
    "    m, s, c, d = mu, sigma, loc, delta\n",
    "    return (\n",
    "        jnp.exp(-(x - m)**2 / 2 / s ** 2) * (2 / jnp.pi)**0.5 * s * (\n",
    "            6 * c * (c + d - m - x)\n",
    "            - 3 * d * (m + x)\n",
    "            + 2 * (m**2 + 2 * s**2 + m * x + x**2)\n",
    "        )\n",
    "        - jax.lax.erf((m - x) / s / 2**0.5) * (\n",
    "            (2 * c + 3 * d - 2 * m) * (c - m)**2\n",
    "            + 3 * s**2 * (2 * c + d - 2 * m)\n",
    "        )\n",
    "    ) / 2 / d**3\n",
    "\n",
    "def bandpass_normal(x, mu, sigma, xmin, xmax, dmin, dmax):\n",
    "    cut = (xmin <= x) * (x <= xmax)\n",
    "    shape = (\n",
    "        jax.scipy.stats.norm.pdf(x, mu, sigma)\n",
    "        * bandpass(x, xmin, xmax, dmin, dmax)\n",
    "    )\n",
    "    norm = (\n",
    "        - normal_integral(xmin, mu, sigma, xmin, dmin)\n",
    "        + normal_integral(xmin + dmin, mu, sigma, xmin, dmin)\n",
    "        - jax.scipy.stats.norm.cdf(xmin + dmin, mu, sigma)\n",
    "        + jax.scipy.stats.norm.cdf(xmax - dmax, mu, sigma)\n",
    "        - normal_integral(xmax - dmax, mu, sigma, xmax, -dmax)\n",
    "        + normal_integral(xmax, mu, sigma, xmax, -dmax)\n",
    "    )\n",
    "    return cut * shape / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83878661-d5fa-4aab-b42a-28994612d456",
   "metadata": {},
   "source": [
    "To build up a population of second-generation mergers, we'll begin by defining the distribution of first-generation mergers. We'll assume that:\n",
    "- black-hole masses are distributed from a tapered power law with an additional Gaussian peak (similar to the [Power Law + Peak](https://arxiv.org/abs/1801.02699) model), where both components decay to zero after the Gaussian peak - this is a mock up of the effect of the (pulsational) pair instability;\n",
    "- black-holes spin magnitudes are drawn from a truncated normal distribution;\n",
    "- spins are isotropic in direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33654819-b3a4-40ef-88a7-d15e6f522cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_mass(parameters):\n",
    "    return parameters['mu_m'] + parameters['sigma_m'] * 3\n",
    "\n",
    "def pdf_m(m, parameters):\n",
    "    pl = bandpass_powerlaw(\n",
    "        m,\n",
    "        alpha = parameters['alpha'],\n",
    "        xmin = parameters['m_min'],\n",
    "        xmax = maximum_mass(parameters),\n",
    "        dmin = parameters['d_min'],\n",
    "        dmax = parameters['sigma_m'] * 3,\n",
    "    )\n",
    "    tn = bandpass_normal(\n",
    "        m,\n",
    "        mu = parameters['mu_m'],\n",
    "        sigma = parameters['sigma_m'],\n",
    "        xmin = parameters['m_min'],\n",
    "        xmax = maximum_mass(parameters),\n",
    "        dmin = parameters['d_min'],\n",
    "        dmax = parameters['sigma_m'] * 3,\n",
    "    )\n",
    "    return (1 - parameters['f_m']) * pl + parameters['f_m'] * tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce83db2-deed-4353-a965-37c2f99c0092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2cb3fc-0b8c-45ad-8f7b-e5df77429da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict(\n",
    "    alpha = -2.5,\n",
    "    m_min = 5,\n",
    "    d_min = 5,\n",
    "    mu_m = 50,\n",
    "    sigma_m = 3,\n",
    "    f_m = 0.1,\n",
    ")\n",
    "\n",
    "m = jnp.linspace(2, 100, 1_000)\n",
    "p = pdf_m(m, parameters)\n",
    "\n",
    "plt.plot(m, p)\n",
    "plt.axvline(parameters['mu_m'], c = 'r', ls = '--')\n",
    "plt.axvline(maximum_mass(parameters), c = 'r', ls = '--')\n",
    "plt.semilogy()\n",
    "plt.ylim(1e-5, 1e0)\n",
    "\n",
    "jnp.trapezoid(p, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c19626-a948-4f64-87ec-d071585cfc12",
   "metadata": {},
   "source": [
    "To generate training data for our model, we need a function to generate samples for our 1G mergers.\n",
    "\n",
    "We model black holes as being paired according to a power law with slope `beta` in the total binary mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18724094-1555-4d87-b1a9-524c607d4b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inversion_sampling(key, shape, pdf, x):\n",
    "    cdf = jnp.cumsum(jnp.diff(x) * (pdf[1:] + pdf[:-1]) / 2)\n",
    "    cdf = jnp.insert(cdf / cdf[-1], 0, 0)\n",
    "    u = jax.random.uniform(key, shape)\n",
    "    return jnp.interp(u, cdf, x)\n",
    "\n",
    "def sample_m(key, shape, parameters):\n",
    "    x = jnp.linspace(parameters['m_min'], maximum_mass(parameters), 1_000)\n",
    "    p = pdf_m(x, parameters)\n",
    "    return inversion_sampling(key, shape, p, x)\n",
    "\n",
    "def sample_truncated_normal(key, shape, mu, sigma, lo, hi):\n",
    "    u = jax.random.uniform(key, shape)\n",
    "    loc = jax.scipy.stats.norm.cdf(lo, mu, sigma)\n",
    "    scale = jax.scipy.stats.norm.cdf(hi, mu, sigma) - loc\n",
    "    return jax.scipy.stats.norm.ppf(u * scale + loc, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76901002-e6fc-4da3-8a6c-59599577100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_1g(key, n, parameters):\n",
    "    # masses\n",
    "    key, subkey = jax.random.split(key)\n",
    "    m = sample_m(subkey, (2, n), parameters)\n",
    "    m1, m2 = m.max(axis = 0), m.min(axis = 0)\n",
    "    q = m2 / m1\n",
    "\n",
    "    # pairing function\n",
    "    pair = (m1 + m2)**parameters['beta']\n",
    "    key, subkey = jax.random.split(key)\n",
    "    idxs = jax.random.choice(subkey, n, shape = (n,), p = pair)\n",
    "    m1, q = m1[idxs], q[idxs]\n",
    "\n",
    "    # spin magnitudes\n",
    "    key, subkey = jax.random.split(key)\n",
    "    a1, a2 = sample_truncated_normal(\n",
    "        subkey, (2, n), parameters['mu_a'], parameters['sigma_a'], 0, 1,\n",
    "    )\n",
    "\n",
    "    # spin tilts\n",
    "    key, subkey = jax.random.split(key)\n",
    "    c1, c2 = jax.random.uniform(subkey, (2, n), minval = -1, maxval = 1)\n",
    "\n",
    "    # azimuthal angle\n",
    "    key, subkey = jax.random.split(key)\n",
    "    dp = jax.random.uniform(subkey, (n,), minval = 0, maxval = 2 * jnp.pi)\n",
    "\n",
    "    return jnp.array([m1, q, a1, a2, c1, c2, dp])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c1e987-5df4-4ac3-a743-0be492dbe071",
   "metadata": {},
   "source": [
    "Next, we want to sample a population of mergers involving a 2G black hole. We'll just consider binaries in which one black hole is 1G and the other is 2G (1G+2G binary).\n",
    "\n",
    "To do so, we need a function to output the properties of black-hole merger remnants from input pre-merger binary properties. Importantly, the remnant recieves a recoil - or \"kick\" - that imparts a velocity, which can eject it from its host environment and thus prevent it from participating in 2G mergers.\n",
    "\n",
    "We follow the fitting formulae to numerical-relativity simulations included in the [precession](https://arxiv.org/abs/1605.01067) Python [package](https://github.com/dgerosa/precession), but with a minimal re-implementation in JAX (I've removed the internal vectorization, which can be done automatically with `jax.vmap`, and fixed to the default spin and kick options)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bdaf07-0ba2-46be-93d1-3193cce67c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_eta(q):\n",
    "    return q / (1 + q)**2\n",
    "\n",
    "def eval_theta12(theta1, theta2, deltaphi):\n",
    "    return jnp.arccos(\n",
    "        jnp.sin(theta1) * jnp.sin(theta2) * jnp.cos(deltaphi)\n",
    "        + jnp.cos(theta1) * jnp.cos(theta2)\n",
    "    )\n",
    "\n",
    "def angles_to_Lframe(theta1, theta2, deltaphi, r, q, chi1, chi2):\n",
    "    L = r**0.5 * q / (1 + q)**2\n",
    "    S1 = chi1 / (1 + q)**2\n",
    "    S2 = chi2 * q**2 / (1 + q)**2\n",
    "\n",
    "    Lx = 0\n",
    "    Ly = 0\n",
    "    Lz = L\n",
    "    Lvec = jnp.array([Lx, Ly, Lz])\n",
    "\n",
    "    S1x = S1 * jnp.sin(theta1)\n",
    "    S1y = 0\n",
    "    S1z = S1 * jnp.cos(theta1)\n",
    "    S1vec = jnp.array([S1x, S1y, S1z])\n",
    "\n",
    "    S2x = S2 * jnp.sin(theta2) * jnp.cos(deltaphi)\n",
    "    S2y = S2 * jnp.sin(theta2) * jnp.sin(deltaphi)\n",
    "    S2z = S2 * jnp.cos(theta2)\n",
    "    S2vec = jnp.array([S2x, S2y, S2z])\n",
    "\n",
    "    return Lvec, S1vec, S2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0831d406-b68a-4c16-9dec-2da019e27bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remnant_mass(theta1, theta2, q, chi1, chi2):\n",
    "    eta = eval_eta(q)\n",
    "\n",
    "    chit_par =  ( chi2*q**2 * jnp.cos(theta2) + chi1*jnp.cos(theta1) ) / (1+q)**2\n",
    "\n",
    "    #Final mass. Barausse Morozova Rezzolla 2012\n",
    "    p0 = 0.04827\n",
    "    p1 = 0.01707\n",
    "    Z1 = 1 + (1-chit_par**2)**(1/3)* ((1+chit_par)**(1/3)+(1-chit_par)**(1/3))\n",
    "    Z2 = (3* chit_par**2 + Z1**2)**(1/2)\n",
    "    risco = 3 + Z2 - jnp.sign(chit_par) * ((3-Z1)*(3+Z1+2*Z2))**(1/2)\n",
    "    Eisco = (1-2/(3*risco))**(1/2)\n",
    "    #Radiated energy, in units of the initial total mass of the binary\n",
    "    Erad = eta*(1-Eisco) + 4* eta**2 * (4*p0+16*p1*chit_par*(chit_par+1)+Eisco-1)\n",
    "    Mfin = 1- Erad # Final mass\n",
    "\n",
    "    return Mfin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a330ff0-8025-4f0d-b98c-738d0f031cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remnant_spin(theta1, theta2, deltaphi, q, chi1, chi2):\n",
    "    eta = eval_eta(q)\n",
    "\n",
    "    kfit = jnp.array( [[jnp.nan, 3.39221, 4.48865, -5.77101, -13.0459] ,\n",
    "                      [35.1278, -72.9336, -86.0036, 93.7371, 200.975],\n",
    "                      [-146.822, 387.184, 447.009, -467.383, -884.339],\n",
    "                      [223.911, -648.502, -697.177, 753.738, 1166.89]])\n",
    "    xifit = 0.474046\n",
    "\n",
    "    # Calculate K00 from Eq 11\n",
    "    kfit = kfit.at[0,0].set(4**2 * ( 0.68646 - jnp.sum( kfit[1:,0] /(4**(3+jnp.arange(kfit.shape[0]-1)))) - (3**0.5)/2))\n",
    "\n",
    "    theta12 = eval_theta12(theta1, theta2, deltaphi)\n",
    "\n",
    "    eps1 = 0.024\n",
    "    eps2 = 0.024\n",
    "    eps12 = 0\n",
    "    theta1 = theta1 + eps1 * jnp.sin(theta1)\n",
    "    theta2 = theta2 + eps2 * jnp.sin(theta2)\n",
    "    theta12 = theta12 + eps12 * jnp.sin(theta12)\n",
    "\n",
    "    # Eq. 14 - 15\n",
    "    atot = ( chi1*jnp.cos(theta1) + chi2*jnp.cos(theta2)*q**2 ) / (1+q)**2\n",
    "    aeff = atot + xifit*eta* ( chi1*jnp.cos(theta1) + chi2*jnp.cos(theta2) )\n",
    "\n",
    "    # Eq. 2 - 6 evaluated at aeff, as specified in Eq. 11\n",
    "    Z1= 1 + (1-(aeff**2))**(1/3) * ( (1+aeff)**(1/3) + (1-aeff)**(1/3) )\n",
    "    Z2= ( (3*aeff**2) + (Z1**2) )**(1/2)\n",
    "    risco= 3 + Z2 - jnp.sign(aeff) * ( (3-Z1)*(3+Z1+2*Z2) )**(1/2)\n",
    "    Eisco=(1-2/(3*risco))**(1/2)\n",
    "    Lisco = (2/(3*(3**(1/2)))) * ( 1 + 2*(3*risco - 2 )**(1/2) )\n",
    "\n",
    "    # Eq. 13\n",
    "    etatoi = eta**(1+jnp.arange(kfit.shape[0]))\n",
    "    innersum = jnp.sum(kfit.T * etatoi,axis=1)\n",
    "    aefftoj = aeff**(jnp.arange(kfit.shape[1]))\n",
    "    sumell = jnp.sum(innersum  * aefftoj,axis=0)\n",
    "    ell = jnp.abs( Lisco  - 2*atot*(Eisco-1)  + sumell )\n",
    "\n",
    "    # Eq. 16\n",
    "    chifin = (1/(1+q)**2) * ( chi1**2 + (chi2**2)*(q**4)  + 2*chi1*chi2*(q**2)*jnp.cos(theta12)\n",
    "            + 2*(chi1*jnp.cos(theta1) + chi2*(q**2)*jnp.cos(theta2))*ell*q + ((ell*q)**2)  )**(1/2)\n",
    "\n",
    "    return jnp.minimum(chifin,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de942ae6-0052-40f5-804d-b873f9b5b2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remnant_kick(bigTheta, theta1, theta2, deltaphi, q, chi1, chi2):\n",
    "# kms=False, maxphase=False, superkick=True, hangupkick=True, crosskick=True, full_output=False):\n",
    "\n",
    "    eta = eval_eta(q)\n",
    "\n",
    "    Lvec, S1vec, S2vec = angles_to_Lframe(theta1, theta2, deltaphi, 1, q, chi1, chi2)\n",
    "    hatL = Lvec / jnp.linalg.norm(Lvec)\n",
    "    hatS1 = S1vec / jnp.linalg.norm(S1vec)\n",
    "    hatS2 = S2vec / jnp.linalg.norm(S2vec)\n",
    "\n",
    "    #More spin parameters.\n",
    "    Delta = - 1/(1+q) * (q*chi2*hatS2 - chi1*hatS1)\n",
    "    Delta_par = jnp.dot(Delta, hatL)\n",
    "    Delta_perp = jnp.linalg.norm(jnp.cross(Delta, hatL))\n",
    "    chit = 1/(1+q)**2 * (chi2*q**2*hatS2 + chi1*hatS1)\n",
    "    chit_par = jnp.dot(chit, hatL)\n",
    "    chit_perp = jnp.linalg.norm(jnp.cross(chit, hatL))\n",
    "\n",
    "    #Coefficients are quoted in km/s\n",
    "    #vm and vperp from Kesden at 2010a. vpar from Lousto Zlochower 2013\n",
    "    zeta=jnp.radians(145)\n",
    "    A=1.2e4\n",
    "    B=-0.93\n",
    "    H=6.9e3\n",
    "\n",
    "    #Multiply by 0/1 boolean flags to select terms\n",
    "    V11 = 3677.76\n",
    "    VA = 2481.21\n",
    "    VB = 1792.45\n",
    "    VC = 1506.52\n",
    "    C2 = 1140\n",
    "    C3 = 2481\n",
    "\n",
    "    # #maxkick\n",
    "    # bigTheta=np.random.uniform(0, 2*np.pi,q.shape) * (not maxphase)\n",
    "\n",
    "    vm = A * eta**2 * (1+B*eta) * (1-q)/(1+q)\n",
    "    vperp = H * eta**2 * Delta_par\n",
    "    vpar = 16*eta**2 * (Delta_perp * (V11 + 2*VA*chit_par + 4*VB*chit_par**2 + 8*VC*chit_par**3) + chit_perp * Delta_par * (2*C2 + 4*C3*chit_par)) * jnp.cos(bigTheta)\n",
    "    kick = jnp.array([vm+vperp*jnp.cos(zeta),vperp*jnp.sin(zeta),vpar]).T\n",
    "\n",
    "    # if not kms:\n",
    "    #     kick = kick/299792.458 # speed of light in km/s\n",
    "\n",
    "    vk = jnp.linalg.norm(kick)\n",
    "\n",
    "    return vk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e541a90-3c9f-4037-97d3-09c9e42f109d",
   "metadata": {},
   "source": [
    "Let's check this matches the original code (note that [precession](https://github.com/dgerosa/precession) resamples the additional angle $\\Theta$ internally, but above we made it an explicit input; we seed the random sampling below to ensure the same values are used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda9d65-b7cc-43a9-894e-532c2c873a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import precession\n",
    "from corner import corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1814d6ed-9bc4-46e2-873b-9cd8f6e2f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10_000\n",
    "np.random.seed(0)\n",
    "bigTheta = np.random.uniform(0, 2 * np.pi, n)\n",
    "theta1 = np.random.uniform(0, np.pi, n)\n",
    "theta2 = np.random.uniform(0, np.pi, n)\n",
    "deltaphi = np.random.uniform(0, 2 * np.pi, n)\n",
    "q = np.random.uniform(0.1, 1, n)\n",
    "chi1 = np.random.uniform(0, 1, n)\n",
    "chi2 = np.random.uniform(0, 1, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeae059d-7842-4e1c-bd85-50ba47db9373",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "og = np.array([\n",
    "    precession.remnantmass(theta1, theta1, q, chi1, chi2),\n",
    "    precession.remnantspin(theta1, theta1, deltaphi, q, chi1, chi2),\n",
    "    precession.remnantkick(theta1, theta1, deltaphi, q, chi1, chi2, kms = True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d43936-26f0-4f26-ab27-c6259d24e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "re = np.array([\n",
    "    jax.vmap(remnant_mass)(theta1, theta1, q, chi1, chi2),\n",
    "    jax.vmap(remnant_spin)(theta1, theta1, deltaphi, q, chi1, chi2),\n",
    "    jax.vmap(remnant_kick)(bigTheta, theta1, theta1, deltaphi, q, chi1, chi2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07950808-6500-47c8-b5c5-8296d29b4144",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(og, re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7483abf2-f7af-4f78-81b9-6847374dbf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = None\n",
    "for i, (samples, ls) in enumerate(zip((og, re), ('-', '--'))):\n",
    "    fig = corner(\n",
    "        samples.T, labels = ('mf', 'af', 'vf'), fig = fig,\n",
    "        plot_datapoints = False, plot_density = False,\n",
    "        plot_contours = True, fill_contours = False, no_fill_contours = True,\n",
    "        hist_kwargs = dict(density = True, color = f'C{i}', ls = ls),\n",
    "        contour_kwargs = dict(colors = [f'C{i}'], linestyles = ls),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a0af2-f873-4627-9867-c2bec285e1b0",
   "metadata": {},
   "source": [
    "Next, we'll combine the 1G population with the remnant fits to produce a population of 1G+2G black-hole mergers.\n",
    "\n",
    "For the 2G black holes, we will assume all mergers occur in an environment with escape speed `v_esc`; remnants that recieve a kick larger than `v_esc` are \"ejected\" and do not lead to 1G+2G mergers.\n",
    "\n",
    "The 2G black holes that are retained will pair with 1G black holes, acording again to a power law in the total mass, this time with slope `gamma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8ffce0-9fa5-4cbe-93c9-c7d3195f7759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remnant(m1, q, a1, a2, c1, c2, dp, th):\n",
    "    t1, t2 = jnp.arccos(c1), jnp.arccos(c2)\n",
    "    mf = remnant_mass(t1, t2, q, a1, a2) * m1 * (1 + q)\n",
    "    af = remnant_spin(t1, t2, dp, q, a1, a2)\n",
    "    vf = remnant_kick(th, t1, t2, dp, q, a1, a2)\n",
    "    return mf, af, vf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73733f02-0e46-4ac6-9f72-23d14f390b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_2g(key, mf, af, vf, parameters):\n",
    "    n = jnp.array([mf, af, vf]).shape[1]\n",
    "\n",
    "    # 1g mass\n",
    "    key, subkey = jax.random.split(key)\n",
    "    m = sample_m(subkey, (n,), parameters)\n",
    "\n",
    "    # ejection and pairing\n",
    "    key, subkey = jax.random.split(key)\n",
    "    pair = (vf < parameters['v_esc']) * (mf + m)**parameters['gamma']\n",
    "    idxs = jax.random.choice(subkey, n, shape = (n,), p = pair)\n",
    "\n",
    "    # masses\n",
    "    m_2g = jnp.stack([mf, m])\n",
    "    sort = jnp.argsort(m_2g, axis = 0)\n",
    "    m2_2g, m1_2g = jnp.take_along_axis(m_2g, sort, axis = 0)\n",
    "    q_2g = m2_2g / m1_2g\n",
    "\n",
    "    # spin magnitudes\n",
    "    key, subkey = jax.random.split(key)\n",
    "    a = sample_truncated_normal(\n",
    "        subkey, (n,), parameters['mu_a'], parameters['sigma_a'], 0, 1,\n",
    "    )\n",
    "    a2_2g, a1_2g = jnp.take_along_axis(jnp.stack([af, a]), sort, axis = 0)\n",
    "\n",
    "    # spin tilts\n",
    "    key, subkey = jax.random.split(key)\n",
    "    c1_2g, c2_2g = jax.random.uniform(subkey, (2, n), minval = -1, maxval = 1)\n",
    "\n",
    "    # azimuthal spin\n",
    "    key, subkey = jax.random.split(key)\n",
    "    dp_2g = jax.random.uniform(subkey, (n,), minval = 0, maxval = 2 * jnp.pi)\n",
    "\n",
    "    return jnp.array([m1_2g, q_2g, a1_2g, a2_2g, c1_2g, c2_2g, dp_2g])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b9636-6ecd-4ec0-8a16-744944f06a0b",
   "metadata": {},
   "source": [
    "Now, to combine these ingredients to sample our overall population.\n",
    "\n",
    "The last assumption we'll make is that the overall fraction of 1G+2G mergers is proportional to the fraction of 2G black holes that are retained (i.e., with kicks smaller than `v_esc`), with constant of proportionality `r_2g`.\n",
    "\n",
    "As we assume that the spin angles are isotropic, we will just model directly the masses and spin magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be10d11-ce2b-418e-bdc9-3a75f496e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf669a-3363-4ce3-a192-88d8d1b708aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mergers(key, n, parameters):\n",
    "    # 1g+1g mergers\n",
    "    key, subkey = jax.random.split(key)\n",
    "    mergers = sample_1g(subkey, n, parameters)\n",
    "\n",
    "    # 1g+1g remnants (2g)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    th = jax.random.uniform(subkey, (n,), minval = 0, maxval = 2 * jnp.pi)\n",
    "    mf, af, vf = jax.vmap(remnant)(*mergers, th)\n",
    "\n",
    "    # 1g+2g mergers\n",
    "    key, subkey = jax.random.split(key)\n",
    "    mergers_2g = sample_2g(subkey, mf, af, vf, parameters)\n",
    "\n",
    "    # mixing\n",
    "    key, subkey = jax.random.split(key)\n",
    "    f = (vf < parameters['v_esc']).mean() * parameters['r_2g']\n",
    "    b = jax.random.binomial(subkey, n = 1, p = f, shape = (n,))\n",
    "    mergers = jnp.where(b, mergers_2g, mergers)\n",
    "\n",
    "    # we'll just fit masses and spin magnitudes as the other angles are\n",
    "    # mostly independent\n",
    "    return mergers[:4].T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec693ab4-5232-4896-889c-5463cf683e3d",
   "metadata": {},
   "source": [
    "Let's check how our simulated model looks on an example.\n",
    "\n",
    "Is this simulated model realistic? No. Will it do for this tutorial? Probably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8076f519-8306-4452-a047-75d567e10630",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict(\n",
    "    alpha = -2.5,\n",
    "    m_min = 5,\n",
    "    d_min = 5,\n",
    "    mu_m = 35,\n",
    "    sigma_m = 3,\n",
    "    f_m = 0.1,\n",
    "    mu_a = 0.1,\n",
    "    sigma_a = 0.2,\n",
    "    beta = 0,\n",
    "    v_esc = 200,\n",
    "    gamma = 0,\n",
    "    r_2g = 0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a230bd-4abd-417e-a651-cd2152c14048",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergers = sample_mergers(jax.random.key(0), 100_000, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da3241f-942d-4f56-9e8c-1233ea5ce892",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = corner(np.array(mergers), bins = 50, labels = ('m1', 'q', 'a1', 'a2'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d8725c-8c42-4c61-b351-59c6e491e9bf",
   "metadata": {},
   "source": [
    "#### Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf62f1ce-fd65-4d86-b2d9-70a1476940c2",
   "metadata": {},
   "source": [
    "Moving on, the next thing we need is a prior distribution of model parameters over which we will sample when training the normalizing flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355660f6-4ae2-4ac5-abdf-8b4c6f5df22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af5f2f-7709-435c-9940-911ff14391fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = dict(\n",
    "    alpha = numpyro.distributions.Uniform(-5, 0),\n",
    "    m_min = numpyro.distributions.Uniform(2, 10),\n",
    "    d_min = numpyro.distributions.Uniform(0, 10),\n",
    "    mu_m = numpyro.distributions.Uniform(20, 50),\n",
    "    sigma_m = numpyro.distributions.Uniform(1, 5),\n",
    "    f_m = numpyro.distributions.Uniform(0, 0.2),\n",
    "    mu_a = numpyro.distributions.Uniform(0, 0.5),\n",
    "    sigma_a = numpyro.distributions.Uniform(0.1, 0.5),\n",
    "    beta = numpyro.distributions.Uniform(-5, 5),\n",
    "    v_esc = numpyro.distributions.Uniform(0, 1_000),\n",
    "    gamma = numpyro.distributions.Uniform(-5, 5),\n",
    "    r_2g = numpyro.distributions.Uniform(0, 0.5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfd917c-cdfc-4634-adb0-1d0c5fcb1b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_dim = len(priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a6977-a756-43c8-8353-67ab784058e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_parameters(key):\n",
    "    keys = jax.random.split(key, len(priors))\n",
    "    return {k: priors[k].sample(key) for k, key in zip(priors, keys)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f09bb50-cb9b-4b9c-9649-bf6210fdfc07",
   "metadata": {},
   "source": [
    "#### Normalizing flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd613a-17a7-451c-b4c1-3757bc4d67f9",
   "metadata": {},
   "source": [
    "Now let's set up the model that we'll train. We'll use a [block neural autoregressive flow](https://arxiv.org/abs/1904.04676) to approximate the simulations. There's a nice library called [flowjax](https://github.com/danielward27/flowjax) to do normalizing flows in JAX that we'll use. This is built on top of [equinox](https://github.com/patrick-kidger/equinox), which will handle our neural networks.\n",
    "\n",
    "If you aren't familiar with normalizing flows, the (very brief) idea is that you can construct a probability distribution by transforming a simple known distribution (such as a standard normal distribution) with an invertible and differentiable function using the change-of-variables formula. For normalizing flows, that function is parametrized by a neural network, which is what makes the transformation flexible.\n",
    "\n",
    "The transformation is trained so that the output distribution best matches some target distribution - in our case the simulation-based population model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fafd15-753f-4177-b4c8-b738a16cea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox\n",
    "from flowjax.distributions import StandardNormal\n",
    "from flowjax.flows import block_neural_autoregressive_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b98422-9335-4c5a-9e83-3e18d1bcf083",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_init = block_neural_autoregressive_flow(\n",
    "    key = jax.random.key(1),\n",
    "    base_dist = StandardNormal(shape = (dim,)),\n",
    "    cond_dim = cond_dim,\n",
    "    invert = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb8a67-0d41-423c-8309-27769b047a98",
   "metadata": {},
   "source": [
    "It works by splitting pytrees - nested python containers of JAX arrays - into trainable and non-trainable subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5c67e-a9d2-4b2a-9907-e1eecfd6be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_init, static = equinox.partition(flow_init, equinox.is_inexact_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb05d4fe-37bf-4f5f-b11e-a251a79ae684",
   "metadata": {},
   "source": [
    "We can easily get the number of parameters (weights and biases of the neural networks) in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c0a9c-72d9-44dc-a33f-b257c94e1c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_init, unravel = jax.flatten_util.ravel_pytree(params_init)\n",
    "array_init.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bbb6c9-f84c-4926-ab81-9cc8f5fde4c5",
   "metadata": {},
   "source": [
    "We should take care that our normalizing flow is defined on the parameter domain we want it to be. In particular, our priors impose bounds on the range of values that can be taken. Therefore, we'll add some additional transformations to ensure those bounds are respected. These transformation are fixed and not trainable, unlike the flow transformations. (We clip the inputs as the bounding transformations are asymptotic, meaning the extremal values are ill defined, e.g., exactly equal masseas.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68726118-8278-459a-85e2-38e6414c8a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "lo = jnp.array([2, 0, 0, 0])\n",
    "hi = jnp.array([200, 1, 1, 1])\n",
    "\n",
    "def inverse_mergers(mergers):\n",
    "    x = (mergers - lo) / (hi - lo)\n",
    "    x = jnp.clip(x, 1e-5, 1 - 1e-5)\n",
    "    return jax.scipy.special.logit(x)\n",
    "\n",
    "def forward_mergers(x):\n",
    "    return jax.nn.sigmoid(x) * (hi - lo) + lo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b86b2c-c9bf-4bc5-b8cd-a04564baf6f9",
   "metadata": {},
   "source": [
    "Though not strictly necessary, we'll also do something similar for the population parameters, so that all inputs to the flow have roughly the same range of numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1070d51-9537-4c37-b530-2bdc8124b746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_parameters(parameters):\n",
    "    x = jnp.array([\n",
    "        (parameters[k] - priors[k].low) / (priors[k].high - priors[k].low)\n",
    "        for k in priors\n",
    "    ])\n",
    "    # x = jnp.clip(x, 1e-5, 1 - 1e-5)\n",
    "    x = jax.scipy.stats.norm.ppf(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9865803-5c07-41c4-99b5-6f2b352febd5",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06041066-7deb-4962-8fa1-c78d36285e58",
   "metadata": {},
   "source": [
    "On to training the model.\n",
    "\n",
    "To train the flow, we need to define a loss function to minimize with respect to the neural-network parameters. The training objective for flows can be thought of in several equivalent ways, including the Kullback-Leibler divergence, the cross entropy, and the likelihood of the training data. The upshot is that the probability density predicted by the flow can be matched to the empirical distribution of the simulations.\n",
    "\n",
    "Below, we define the loss function and training loop to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc49b5a-01e2-41bd-8af6-3baac972d832",
   "metadata": {},
   "source": [
    "First, let's set some training parameters. We'll use [optax](https://github.com/google-deepmind/optax) to update the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f73c7-79e9-4e7f-b377-731f8bc7d98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed15ead-1400-4771-a897-fa7791b0cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1_000\n",
    "steps = 10_000\n",
    "learning_rate = 1e-2\n",
    "learning_rate = optax.cosine_decay_schedule(learning_rate, steps)\n",
    "optimizer = optax.adam(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c341e8-1ca7-4844-9883-faa2a5eb3b1c",
   "metadata": {},
   "source": [
    "*Note that we will train the flow by showing it only one simulation at a time, i.e., one value of `parameters` that produces an amount `batch_size` of mergers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4bd201-4c53-4979-937f-40bd75a5b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap the sampling and bounding functions together\n",
    "def sample(key):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    parameters = sample_parameters(subkey)\n",
    "    c = inverse_parameters(parameters)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    mergers = sample_mergers(subkey, batch_size, parameters)\n",
    "    x = inverse_mergers(mergers)\n",
    "    return x, c\n",
    "\n",
    "# the average likelihood of mergers from a single simulation\n",
    "def loss_fn(params, key):\n",
    "    flow = equinox.combine(params, static)\n",
    "    x, c = sample(key)\n",
    "    return -flow.log_prob(x, c).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d144f-9f52-48d1-8b9c-0e6761ab38cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if things go wrong and the loss becomes non-finite,\n",
    "# then return to the best last update\n",
    "def check_finite(carry, loss):\n",
    "    key, params, state, best_params, best_state, best_loss = carry\n",
    "    params, state = jax.lax.cond(\n",
    "        jnp.isfinite(loss),\n",
    "        lambda: (params, state),\n",
    "        lambda: (best_params, best_state),\n",
    "    )\n",
    "    return key, params, state, best_params, best_state, best_loss\n",
    "\n",
    "# keep track of the flow with the best loss value\n",
    "def check_best(carry, loss):\n",
    "    key, params, state, best_params, best_state, best_loss = carry\n",
    "    best_params, best_state, best_loss = jax.lax.cond(\n",
    "        loss < best_loss,\n",
    "        lambda: (params, state, loss),\n",
    "        lambda: (best_params, best_state, best_loss),\n",
    "    )\n",
    "    return key, params, state, best_params, best_state, best_loss\n",
    "\n",
    "# function to update the neural-network parameters\n",
    "def update(carry, step):\n",
    "    key, params, state, best_params, best_state, best_loss = carry\n",
    "    key, subkey = jax.random.split(key)\n",
    "    loss, grad = equinox.filter_value_and_grad(loss_fn)(params, subkey)\n",
    "    updates, state = optimizer.update(grad, state, params)\n",
    "    params = equinox.apply_updates(params, updates)\n",
    "    carry = key, params, state, best_params, best_state, best_loss\n",
    "    carry = check_finite(carry, loss)\n",
    "    carry = check_best(carry, loss)\n",
    "    return carry, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dadd38-8ce5-4985-af53-196b75a26fba",
   "metadata": {},
   "source": [
    "Another handy package is [jax_tqdm](https://github.com/jeremiecoullon/jax-tqdm) to add progress bar to JAX loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad6cb2-6b4a-45d9-9f78-0dffcae5d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e54bc-992c-4b8c-bc14-99d3f7aba6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "update = jax_tqdm.scan_tqdm(steps, print_rate = 100, tqdm_type = 'std')(update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33e6c2f-6a20-4918-a8c4-2d133fdf43a6",
   "metadata": {},
   "source": [
    "Now let's train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6b0437-4117-493b-9995-23d5c805ebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, the training loop (written without Python loops using JAX)\n",
    "state = optimizer.init(params_init)\n",
    "carry = (\n",
    "    jax.random.key(2),\n",
    "    params_init,\n",
    "    state,\n",
    "    params_init,\n",
    "    state,\n",
    "    jnp.inf,\n",
    ")\n",
    "carry, losses = jax.lax.scan(update, carry, jnp.arange(steps))\n",
    "key, params, state, best_params, best_state, best_loss = carry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424119f0-cb50-4b1b-bc6f-f9080cc10129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss values over training steps\n",
    "plt.plot(losses);\n",
    "plt.axvline(losses.argmin(), c = 'r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7ba360-12ad-4662-8096-e22c01c6ffb8",
   "metadata": {},
   "source": [
    "We can reconstruct the flow at both the last training step and the step with the best loss (but note that, due to sampling noise, the lowest loss value is not necessarily the one producing the \"best\" trained model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb601a21-c81e-49b2-851f-dbdbd7f89b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_flow = equinox.combine(params, static)\n",
    "best_flow = equinox.combine(best_params, static)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa39459-c9dd-43b3-90ad-e482a32bb5dc",
   "metadata": {},
   "source": [
    "How did the training do? Let's compare the trained flow to the actual simulated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c407ed-b185-4797-879e-74a99dce6554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corner import corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc5a292-2d69-466c-b0b4-741476572d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to select particular population parameters, uncomment this\n",
    "# parameters = dict(\n",
    "#     alpha = -2.5,\n",
    "#     m_min = 5,\n",
    "#     d_min = 5,\n",
    "#     mu_m = 35,\n",
    "#     sigma_m = 3,\n",
    "#     f_m = 0.1,\n",
    "#     mu_a = 0.1,\n",
    "#     sigma_a = 0.2,\n",
    "#     beta = 0,\n",
    "#     v_esc = 200,\n",
    "#     gamma = 0,\n",
    "#     r_2g = 0.2,\n",
    "# )\n",
    "\n",
    "# # to select random population parameters, uncomment this\n",
    "parameters = sample_parameters(jax.random.key(np.random.randint(1e9)))\n",
    "\n",
    "c = inverse_parameters(parameters)\n",
    "\n",
    "mergers = sample_mergers(jax.random.key(4), 10_000, parameters)\n",
    "\n",
    "x = last_flow.sample(jax.random.key(5), (10_000,), condition = c)\n",
    "last_flow_mergers = forward_mergers(x)\n",
    "\n",
    "x = best_flow.sample(jax.random.key(6), (10_000,), condition = c)\n",
    "best_flow_mergers = forward_mergers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d7f9db-b015-4a9e-abb5-db20d919df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd702cf5-4187-474c-bbb8-4b09df07a9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lim = np.max([\n",
    "    mergers[:, 0].max(),\n",
    "    last_flow_mergers[:, 0].max(),\n",
    "    best_flow_mergers[:, 0].max(),\n",
    "])\n",
    "bounds = np.transpose([lo, hi])\n",
    "bounds[0, 1] = lim\n",
    "\n",
    "fig = None\n",
    "for i, samples in enumerate((mergers, last_flow_mergers, best_flow_mergers)):\n",
    "    fig = corner(\n",
    "        np.array(samples), labels = ('m1', 'q', 'a1', 'a2'),\n",
    "        fig = fig, plot_datapoints = False, plot_density = False,\n",
    "        plot_contours = True, fill_contours = False, no_fill_contours = True,\n",
    "        hist_kwargs = dict(density = True, color = f'C{i}'),\n",
    "        contour_kwargs = dict(colors = [f'C{i}']),\n",
    "        range = bounds,\n",
    "        levels = (0.5, 0.9, 0.99), bins = 20, smooth = 0.5,\n",
    "    )\n",
    "\n",
    "for i in range(dim):\n",
    "    ax = fig.axes[i + dim * i]\n",
    "    ylim = max(patch.xy[:, 1].max() for patch in ax.patches)\n",
    "    ax.set_ylim(0, ylim * 1.1)\n",
    "\n",
    "fig.axes[1].legend(\n",
    "    handles = fig.axes[0].patches,\n",
    "    labels = ('simulation', 'last flow', 'best flow'),\n",
    "    fontsize = 20,\n",
    "    loc = 'upper left',\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6bcbfd-56ca-4e3f-baff-97c056df7870",
   "metadata": {},
   "source": [
    "#### Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50666f-6b7c-4026-aaaf-f31ad9a548ae",
   "metadata": {},
   "source": [
    "Now you've trained the population model, what next?\n",
    "- Combine the implementation of the likelihood and sampling algorithms from other notebooks to infer the population parameters of our model.\n",
    "- Run the original simulated model and the trained flow using the population inference results to see how predictions compare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7253dcf9-9972-42c6-995c-536ec4d158ed",
   "metadata": {},
   "source": [
    "We should add a bit more realism to our model:\n",
    "- Add a model for the evolution of the merger rate over redshift.\n",
    "- Account for differing time delays and redshift evolution for first- and second-generaiton mergers.\n",
    "- Allow for a subpopulation of binaries in which both black holes are second generation, or high generation.\n",
    "- Allow for subpopulation form other channels with distinct first-generation properties, e.g., isolated evolution where second-generation mergers don't occur.\n",
    "- Improve the prescriptions for binary pairing.\n",
    "- Include a more realistic model for the host environments and their escape speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e627ac6f-df37-44b7-9626-912ee18741bf",
   "metadata": {},
   "source": [
    "There is some more immediate tinkering you can do with the training code above:\n",
    "- The training settings, e.g., batch size, number of training steps, learning rate, optimizer etc. Try a [learning-rate scheduler](https://optax.readthedocs.io/en/latest/api/optimizer_schedules.html), for example.\n",
    "- The [flow settings](https://danielward27.github.io/flowjax/api/flows.html#flowjax.flows.block_neural_autoregressive_flow), e.g., try making the network smaller or larger.\n",
    "- The flow itself, i.e., try [a different type](https://danielward27.github.io/flowjax/api/flows.html) of normalizing flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81932ee-f0c3-4be5-b659-6f1a1a84aedc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
